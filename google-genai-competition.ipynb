{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac01979",
   "metadata": {
    "papermill": {
     "duration": 0.006242,
     "end_time": "2025-04-13T19:33:44.220837",
     "exception": false,
     "start_time": "2025-04-13T19:33:44.214595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Scientific Paper Recommendation System with RAG\n",
    "## Project Description\n",
    "This project implements a Retrieval Augmented Generation (RAG) system for scientific paper recommendations. The system allows users to input a document or query and receive recommendations for relevant scientific papers from the ArXiv database.\n",
    "\n",
    "## Key Components\n",
    "Vector Database: Uses a ChromaDB vector database containing embeddings of 10,000 ArXiv research papers\n",
    "Document Processing: Extracts and processes PDF content using PyMuPDF\n",
    "Semantic Search: Performs similarity searches based on document content\n",
    "\n",
    "## GenAI Functionalities\n",
    "- Embeddings: Generates semantic embeddings using Google's text-embedding-004 model\n",
    "- Prompt Engineering: Utilizes carefully crafted prompts to guide the AI's behavior\n",
    "- RAG Implementation: Combines vector search results with generative AI responses\n",
    "- Document Understanding: Processes and interprets PDF research papers\n",
    "- Vector Embedding and Vector Search: Performs semantic similarity searches in high-dimensional vector space\n",
    "\n",
    "The system orchestrates these components through a chatbot interface that processes user queries, searches for relevant papers, and generates comprehensive responses that include paper details like authors and publication dates.\n",
    "\n",
    "## Database\n",
    "ArXiv serves as an excellent data source for our recommendation system for several key reasons:\n",
    "\n",
    "- **Rich Scientific Content**: Contains over 2 million scholarly articles across multiple disciplines\n",
    "- **Well-Structured Metadata**: Includes titles, abstracts, authors, and categories in a consistent format\n",
    "- **Embedding-Friendly**: Abstracts provide concise, information-dense text that produces meaningful vector embeddings\n",
    "- **Research Relevance**: Widely used by the scientific community, ensuring practical utility\n",
    "- **Semantic Search Compatibility**: Content structure works effectively with our embedding model (text-embedding-004)\n",
    "\n",
    "Our implementation uses 10,000 ArXiv papers converted to vector embeddings and stored in ChromaDB, enabling semantic similarity searches to retrieve relevant scientific literature for user queries. The Arxiv dataset is available in Kagggle with the following link: [Arxiv_dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv). \n",
    "## Use Case\n",
    "Researchers can upload a scientific paper and ask questions to find related work in the ArXiv database, facilitating literature reviews and discovery of relevant research.\n",
    "\n",
    "````\n",
    "chatbot = RAG_Scientific_chatbot()\n",
    "answer = chatbot.chat(\"Find me related papers\", \"/path/to/document.pdf\")\n",
    "display(Markdown(answer))\n",
    "```` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f333cf8",
   "metadata": {
    "papermill": {
     "duration": 0.005244,
     "end_time": "2025-04-13T19:33:44.231731",
     "exception": false,
     "start_time": "2025-04-13T19:33:44.226487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "Import and install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a790fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:33:44.243965Z",
     "iopub.status.busy": "2025-04-13T19:33:44.243528Z",
     "iopub.status.idle": "2025-04-13T19:34:35.647627Z",
     "shell.execute_reply": "2025-04-13T19:34:35.646419Z"
    },
    "papermill": {
     "duration": 51.412335,
     "end_time": "2025-04-13T19:34:35.649498",
     "exception": false,
     "start_time": "2025-04-13T19:33:44.237163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.9/454.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\r\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mCollecting pymupdf\r\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\r\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pymupdf\r\n",
      "Successfully installed pymupdf-1.25.5\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n",
    "!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\"\n",
    "!pip install --upgrade pymupdf\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46105f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:34:35.668030Z",
     "iopub.status.busy": "2025-04-13T19:34:35.667486Z",
     "iopub.status.idle": "2025-04-13T19:34:36.084810Z",
     "shell.execute_reply": "2025-04-13T19:34:36.083733Z"
    },
    "papermill": {
     "duration": 0.428707,
     "end_time": "2025-04-13T19:34:36.086834",
     "exception": false,
     "start_time": "2025-04-13T19:34:35.658127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API keys\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19964660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:34:36.105269Z",
     "iopub.status.busy": "2025-04-13T19:34:36.104892Z",
     "iopub.status.idle": "2025-04-13T19:34:36.349980Z",
     "shell.execute_reply": "2025-04-13T19:34:36.348946Z"
    },
    "papermill": {
     "duration": 0.256762,
     "end_time": "2025-04-13T19:34:36.351898",
     "exception": false,
     "start_time": "2025-04-13T19:34:36.095136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a retry policy. The model might make multiple consecutive calls automatically\n",
    "# for a complex query, this ensures the client retries if it hits quota limits.\n",
    "from google.api_core import retry\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "if not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n",
    "  genai.models.Models.generate_content = retry.Retry(\n",
    "      predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e39b5c2",
   "metadata": {
    "papermill": {
     "duration": 0.008189,
     "end_time": "2025-04-13T19:34:36.368455",
     "exception": false,
     "start_time": "2025-04-13T19:34:36.360266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Upload of Arxiv papers\n",
    "First the arxiv dataset is imported and a vector embedding of all the documents is perfomed. After the vector embedding,these are stored in a chromadb vector database. The arxiv dataset import is shown below. Given that it is computationally expensive to import and process all papers, only a few are imported randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95413fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:34:36.386888Z",
     "iopub.status.busy": "2025-04-13T19:34:36.386317Z",
     "iopub.status.idle": "2025-04-13T19:36:41.181660Z",
     "shell.execute_reply": "2025-04-13T19:36:41.180492Z"
    },
    "papermill": {
     "duration": 124.814514,
     "end_time": "2025-04-13T19:36:41.191507",
     "exception": false,
     "start_time": "2025-04-13T19:34:36.376993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "amount_papers = 10000\n",
    "papers = []\n",
    "\n",
    "with open('/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json', 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        papers.append(json.loads(line))\n",
    "\n",
    "random_indices = set(random.sample(range(len(papers)), amount_papers))\n",
    "random_indices = list(random_indices)\n",
    "papers_random = []\n",
    "for i in range(len(random_indices)):\n",
    "    index = random_indices[i]\n",
    "    papers_random.append(papers[index])\n",
    "papers = papers_random\n",
    "# Now data is a list of dictionaries\n",
    "print(\"Headers:\", list(papers[0].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4cf999",
   "metadata": {
    "papermill": {
     "duration": 0.007937,
     "end_time": "2025-04-13T19:36:41.207924",
     "exception": false,
     "start_time": "2025-04-13T19:36:41.199987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Only the title and the abstract of each paper are embedded. The code below implements this preprocessing of the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae81a91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:36:41.359045Z",
     "iopub.status.busy": "2025-04-13T19:36:41.358639Z",
     "iopub.status.idle": "2025-04-13T19:36:41.450610Z",
     "shell.execute_reply": "2025-04-13T19:36:41.449531Z"
    },
    "papermill": {
     "duration": 0.236226,
     "end_time": "2025-04-13T19:36:41.452316",
     "exception": false,
     "start_time": "2025-04-13T19:36:41.216090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFULLY PREPROCESSED 10000 PAPERS\n",
      "--- EXAMPLE OF PREPROCESSED PAPER ---\n",
      "PAPER TITLE: Scaling of Percolation on Infinite Planar Maps, I\n",
      "PAPER CONTENT:   We consider several aspects of the scaling limit of percolation on random planar triangulations, both finite and infinite. The equivalents for random maps of Cardy's formula for the limit under scaling of various crossing probabilities are given. The limit probabilities are expressed in terms of simple events regarding Airy-Levy processes. Some explicit formulas for limit probabilities follow from this relation by applying known results on stable processes. Conversely, natural symmetries of the random maps imply identities concerning the Airy-Levy processes. \n"
     ]
    }
   ],
   "source": [
    "def remove_newlines(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return obj.replace('\\n', ' ')\n",
    "        \n",
    "preprocessed_papers = []\n",
    "for paper in papers:\n",
    "    preprocessed_papers.append(\"PAPER TITLE: \" + remove_newlines(paper[\"title\"]) + \"\\nPAPER CONTENT: \"+ remove_newlines(paper[\"abstract\"]))\n",
    "print(\"SUCCESSFULLY PREPROCESSED \"+ str(len(preprocessed_papers)) + \" PAPERS\")\n",
    "print(\"--- EXAMPLE OF PREPROCESSED PAPER ---\")\n",
    "print(preprocessed_papers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09242c0",
   "metadata": {
    "papermill": {
     "duration": 0.008144,
     "end_time": "2025-04-13T19:36:41.469158",
     "exception": false,
     "start_time": "2025-04-13T19:36:41.461014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now the preprocessed papers are transformed into vector embeddings with the embedding model  models/text-embedding-004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ba5d31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:36:41.488191Z",
     "iopub.status.busy": "2025-04-13T19:36:41.487836Z",
     "iopub.status.idle": "2025-04-13T19:38:07.982187Z",
     "shell.execute_reply": "2025-04-13T19:38:07.980911Z"
    },
    "papermill": {
     "duration": 86.512102,
     "end_time": "2025-04-13T19:38:07.990547",
     "exception": false,
     "start_time": "2025-04-13T19:36:41.478445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFULLY EMBEDDED 10000 PAPERS\n"
     ]
    }
   ],
   "source": [
    "def batch(iterable, n=100):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i + n]\n",
    "\n",
    "papers_embedded = []  \n",
    "papers_batches = list(batch(preprocessed_papers, 100)) #limit of 100 embeddings per call\n",
    "\n",
    "for batch in papers_batches:\n",
    "    batch_embedded = client.models.embed_content(\n",
    "        model='models/text-embedding-004',\n",
    "        contents=batch,\n",
    "        config=types.EmbedContentConfig(task_type='SEMANTIC_SIMILARITY'))\n",
    "    list_batch_embedded = [e.values for e in batch_embedded.embeddings]\n",
    "    papers_embedded+=list_batch_embedded\n",
    "\n",
    "print(\"SUCCESSFULLY EMBEDDED \"+ str(len(papers_embedded)) + \" PAPERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4979db",
   "metadata": {
    "papermill": {
     "duration": 0.008167,
     "end_time": "2025-04-13T19:38:08.007194",
     "exception": false,
     "start_time": "2025-04-13T19:38:07.999027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Once the vector embeddings of the papers are computed, these are stored into the chromadb database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324cb828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:38:08.025116Z",
     "iopub.status.busy": "2025-04-13T19:38:08.024745Z",
     "iopub.status.idle": "2025-04-13T19:38:27.444783Z",
     "shell.execute_reply": "2025-04-13T19:38:27.443624Z"
    },
    "papermill": {
     "duration": 19.430985,
     "end_time": "2025-04-13T19:38:27.446489",
     "exception": false,
     "start_time": "2025-04-13T19:38:08.015504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFULLY UPLOADED 10000 PAPERS\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "def batch(iterable, batch_size):\n",
    "    for i in range(0, len(iterable), batch_size):\n",
    "        yield iterable[i:i + batch_size]\n",
    "\n",
    "\n",
    "# Start ChromaDB client\n",
    "chromadb_client = chromadb.Client()\n",
    "\n",
    "# Create or get a collection\n",
    "collection = chromadb_client.get_or_create_collection(name=\"papers\")\n",
    "\n",
    "# Add the documents + embeddings to Chroma\n",
    "emb_batches = list(batch(papers_embedded, 41000))\n",
    "papers_batches = list(batch(preprocessed_papers, 41000))\n",
    "for i in range(len(emb_batches)):\n",
    "    ids_batch = [f\"doc_{j + i * 41000}\" for j in range(len(emb_batches[i]))]\n",
    "    collection.add(\n",
    "        documents=papers_batches[i],\n",
    "        embeddings=emb_batches[i],\n",
    "        ids=ids_batch,\n",
    "    )\n",
    "print(\"SUCCESSFULLY UPLOADED \"+ str(len(papers_embedded)) + \" PAPERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb0efd",
   "metadata": {
    "papermill": {
     "duration": 0.008156,
     "end_time": "2025-04-13T19:38:27.463414",
     "exception": false,
     "start_time": "2025-04-13T19:38:27.455258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vector database search example\n",
    "\n",
    "Now an extract of a sample paper is used to search for similar papers in the database. If the same paper is obtained, the queried paper was in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efaa5315",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:38:27.481865Z",
     "iopub.status.busy": "2025-04-13T19:38:27.481465Z",
     "iopub.status.idle": "2025-04-13T19:38:27.861189Z",
     "shell.execute_reply": "2025-04-13T19:38:27.860078Z"
    },
    "papermill": {
     "duration": 0.391249,
     "end_time": "2025-04-13T19:38:27.863109",
     "exception": false,
     "start_time": "2025-04-13T19:38:27.471860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_input = \"Statistical modeling of experimental physical laws is based on the probability density function of measured variables. It is expressed by experimental data via a kernel estimator. The kernel is determined objectively by the scattering of data during calibration of experimental setup. A physical law, which relates measured variables, is optimally extracted from experimental data by the conditional average estimator. It is derived directly from the kernel estimator and corresponds to a general nonparametric regression. T\"\n",
    "#query_input = pdf_text\n",
    "\n",
    "query_embedding = client.models.embed_content(\n",
    "        model='models/text-embedding-004',\n",
    "        contents=query_input,\n",
    "        config=types.EmbedContentConfig(task_type='SEMANTIC_SIMILARITY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c23e3f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:38:27.881475Z",
     "iopub.status.busy": "2025-04-13T19:38:27.881120Z",
     "iopub.status.idle": "2025-04-13T19:38:27.895253Z",
     "shell.execute_reply": "2025-04-13T19:38:27.894094Z"
    },
    "papermill": {
     "duration": 0.025529,
     "end_time": "2025-04-13T19:38:27.897241",
     "exception": false,
     "start_time": "2025-04-13T19:38:27.871712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: doc_2434\n",
      "PAPER TITLE: Feature calibration for computer models\n",
      "PAPER CONTENT:   Computer model calibration involves using partial and imperfect observations of the real world to learn which values of a model's input parameters lead to outputs that are consistent with real-world observations. When calibrating models with high-dimensional output (e.g. a spatial field), it is common to represent the output as a linear combination of a small set of basis vectors. Often, when trying to calibrate to such output, what is important to the credibility of the model is that key emergent physical phenomena are represented, even if not faithfully or in the right place. In these cases, comparison of model output and data in a linear subspace is inappropriate and will usually lead to poor model calibration. To overcome this, we present kernel-based history matching (KHM), generalising the meaning of the technique sufficiently to be able to project model outputs and observations into a higher-dimensional feature space, where patterns can be compared without their location necessarily being fixed. We develop the technical methodology, present an expert-driven kernel selection algorithm, and then apply the techniques to the calibration of boundary layer clouds for the French climate model IPSL-CM. \n",
      "\n",
      "ID: doc_7675\n",
      "PAPER TITLE: Differentiable Calibration of Inexact Stochastic Simulation Models via   Kernel Score Minimization\n",
      "PAPER CONTENT:   Stochastic simulation models are generative models that mimic complex systems to help with decision-making. The reliability of these models heavily depends on well-calibrated input model parameters. However, in many practical scenarios, only output-level data are available to learn the input model parameters, which is challenging due to the often intractable likelihood of the stochastic simulation model. Moreover, stochastic simulation models are frequently inexact, with discrepancies between the model and the target system. No existing methods can effectively learn and quantify the uncertainties of input parameters using only output-level data. In this paper, we propose to learn differentiable input parameters of stochastic simulation models using output-level data via kernel score minimization with stochastic gradient descent. We quantify the uncertainties of the learned input parameters using a frequentist confidence set procedure based on a new asymptotic normality result that accounts for model inexactness. The proposed method is evaluated on exact and inexact G/G/1 queueing models. \n",
      "\n",
      "ID: doc_2510\n",
      "PAPER TITLE: The LASSO with Non-linear Measurements is Equivalent to One With Linear   Measurements\n",
      "PAPER CONTENT:   Consider estimating an unknown, but structured, signal $x_0\\in R^n$ from $m$ measurement $y_i=g_i(a_i^Tx_0)$, where the $a_i$'s are the rows of a known measurement matrix $A$, and, $g$ is a (potentially unknown) nonlinear and random link-function. Such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties. It could also arise by design, e.g., $g_i(x)=\\text{sign}(x+z_i)$, corresponds to noisy 1-bit quantized measurements. Motivated by the classical work of Brillinger, and more recent work of Plan and Vershynin, we estimate $x_0$ via solving the Generalized-LASSO for some regularization parameter $\\lambda>0$ and some (typically non-smooth) convex structure-inducing regularizer function. While this approach seems to naively ignore the nonlinear function $g$, both Brillinger (in the non-constrained case) and Plan and Vershynin have shown that, when the entries of $A$ are iid standard normal, this is a good estimator of $x_0$ up to a constant of proportionality $\\mu$, which only depends on $g$. In this work, we considerably strengthen these results by obtaining explicit expressions for the squared error, for the \\emph{regularized} LASSO, that are asymptotically \\emph{precise} when $m$ and $n$ grow large. A main result is that the estimation performance of the Generalized LASSO with non-linear measurements is \\emph{asymptotically the same} as one whose measurements are linear $y_i=\\mu a_i^Tx_0 + \\sigma z_i$, with $\\mu = E\\gamma g(\\gamma)$ and $\\sigma^2 = E(g(\\gamma)-\\mu\\gamma)^2$, and, $\\gamma$ standard normal. To the best of our knowledge, the derived expressions on the estimation performance are the first-known precise results in this context. One interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the LASSO is the celebrated Lloyd-Max quantizer. \n",
      "\n",
      "ID: doc_5637\n",
      "PAPER TITLE: Reduced variable optimization methods via implicit functional dependence   with applications\n",
      "PAPER CONTENT:   Optimization methods have been broadly applied to two classes of objects viz. (i) modeling and description of data and (ii) the determination of the stationary points of functions. Here, a theoretical basis is developed that optimizes an arbitrary number of variables for classes (i) and (ii) by the minimization of a function of a single variable. Algorithms that focus on a reduced variable set also avoid problems associated with multiple minima and maxima that arise because of the large numbers of parameters. The methods described could have applications in the physical sciences where the optimization of one physically significant variable has priority over the other variables. For (i), we develop both an approximate but computationally more tractable method and an exact method where the single controlling variable k of all the other variables (P,k) passes through the local stationary point of the least squares (LS) metric. For (ii), an exact theory is developed whereby the optimized function of an independent variation of all parameters coincides with that due to single parameter optimization. The implicit function theorem has to be further qualified to arrive at this result. The topology of the surfaces of constant value of the target or cost function are considered for all the methods. A real world application of the above implicit methodology to rate constant and final concentration parameter determination for first and second order chemical reactions from published data. This work is different from and more general than all the reduction schemes for conditional linear parameters nor is it a subset of the Adomian decomposition method (ADM) used for estimating solutions of differential equations, which still require boundary conditions that do not feature in topics (i) and (ii). \n",
      "\n",
      "ID: doc_9859\n",
      "PAPER TITLE: Variable bandwidth kernel regression estimation\n",
      "PAPER CONTENT:   In this paper we propose a variable bandwidth kernel regression estimator for $i.i.d.$ observations in $\\mathbb{R}^2$ to improve the classical Nadaraya-Watson estimator. The bias is improved to the order of $O(h_n^4)$ under the condition that the fifth order derivative of the density function and the sixth order derivative of the regression function are bounded and continuous. We also establish the central limit theorems for the proposed ideal and true variable kernel regression estimators. The simulation study confirms our results and demonstrates the advantage of the variable bandwidth kernel method over the classical kernel method. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding.embeddings[0].values],\n",
    "    n_results=5  # Number of similar docs to return\n",
    ")\n",
    "\n",
    "for doc, doc_id in zip(results[\"documents\"][0], results[\"ids\"][0]):\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93dfa9",
   "metadata": {
    "papermill": {
     "duration": 0.008135,
     "end_time": "2025-04-13T19:38:27.914238",
     "exception": false,
     "start_time": "2025-04-13T19:38:27.906103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RETRIEVAL AUGMENTED GENERATION (RAG)\n",
    "For retrieval augmented generation, the question of the user together with the document are used to search for useful papers. With the useful papers and the user input, an answer is generated. The steps are as follow:\n",
    "1) Use a LLM to embed the user input question and input document for vector search.\n",
    "2) Obtain the original documents from the vector search in the database.\n",
    "3) Use the input question and input document and the original documents from the database to generate a response with a LLM.\n",
    "4) Show the answer to the user.\n",
    "\n",
    "Prompt engineering is employed to adjust the answers of the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606c832",
   "metadata": {
    "papermill": {
     "duration": 0.008636,
     "end_time": "2025-04-13T19:38:27.931211",
     "exception": false,
     "start_time": "2025-04-13T19:38:27.922575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Orchestration functions\n",
    "The following functions orchestrate the RAG and found in the class RAG_Scientific_chatbot:\n",
    "\n",
    "- _create_embedding(text): For a given text generates the corresponding vector embedding.\n",
    "- _search_embedded_documents(query_embedding, n): For a given vector, searches nearby vectors in the vector embeddings database.\n",
    "- _retrieve_documents(doc_id): For a given list of document ids, this function returns an extended information of each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b08fc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:38:27.949889Z",
     "iopub.status.busy": "2025-04-13T19:38:27.949487Z",
     "iopub.status.idle": "2025-04-13T19:38:28.050754Z",
     "shell.execute_reply": "2025-04-13T19:38:28.049599Z"
    },
    "papermill": {
     "duration": 0.112823,
     "end_time": "2025-04-13T19:38:28.052650",
     "exception": false,
     "start_time": "2025-04-13T19:38:27.939827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import re\n",
    "\n",
    "class RAG_Scientific_chatbot:\n",
    "        \n",
    "    def chat(self, question:str, document_path:str=None):\n",
    "                           \n",
    "        processed_input, user_document = self._process_input(question, document_path)\n",
    "    \n",
    "        embedding = self._create_embedding(processed_input[\"embedding_query\"])\n",
    "        \n",
    "        search_output = self._search_embedded_documents(embedding, int(processed_input[\"num_documents\"]))\n",
    "        \n",
    "        doc_ids = search_output['ids'][0]  \n",
    "        numeric_ids = [int(doc.split('_')[1]) for doc in doc_ids]\n",
    "        extended_info = self._retrieve_documents(numeric_ids)\n",
    "        answer = self._generate_final_answer(question, user_document, search_output, extended_info)\n",
    "\n",
    "        return answer\n",
    "\n",
    "\n",
    "    # === Tools ===\n",
    "    def _process_input(self, user_message: str, document_path:str = None ):\n",
    "        instruction = \"\"\"\n",
    "            You are a scientific research assistant specializing in analyzing academic papers and research questions.\n",
    "            \n",
    "            Your task is to analyze the user's question and their uploaded document to create:\n",
    "            1. An optimal embedding query for retrieving the most relevant scientific papers\n",
    "            2. A recommendation for how many papers to retrieve\n",
    "            \n",
    "            INSTRUCTIONS:\n",
    "            - Identify key scientific concepts, methodologies, domain-specific terminology, and research areas\n",
    "            - Extract specific technical terms that would appear in related papers\n",
    "            - Consider both the user's explicit question and the implicit research goals from their document\n",
    "            - Focus on scientific significance rather than general terms\n",
    "            - For empirical research questions, include methodology terms and measurement concepts\n",
    "            - For theoretical questions, include relevant frameworks and paradigms\n",
    "            \n",
    "            For number of documents:\n",
    "            - Suggest 3-5 papers for focused questions with specific methodology/technology\n",
    "            - Suggest 5-8 papers for broader research areas requiring multiple perspectives\n",
    "            - Suggest 8-12 papers for literature reviews or comparative analyses\n",
    "            - If the amount of papers is explicit in QUESTION, search for this amount of papers or more\n",
    "            \n",
    "            \n",
    "            Return this string output:\n",
    "            \"{\"embedding_query\": \"<your optimized embedding query>\",\n",
    "             \"num_documents\": \"<number of papers to retrieve>\"}\"\n",
    "            \n",
    "            EXAMPLES:\n",
    "            \n",
    "            Question: Find me 3 papers about science and 2 about nature.  \n",
    "            Good num_documents: 7\n",
    "            Bad num_documents: 3\n",
    "            \n",
    "            \"\"\"\n",
    "\n",
    "        if document_path != None:\n",
    "            pdf_text = self._extract_text_from_pdf(document_path)\n",
    "            contents = [\n",
    "                types.Content(\n",
    "                    role=\"user\", parts=[types.Part(text=user_message),types.Part(text=pdf_text)]\n",
    "                )\n",
    "            ]\n",
    "          \n",
    "        else: \n",
    "            pdf_text = None\n",
    "            contents = [\n",
    "                types.Content(\n",
    "                    role=\"user\", parts=[types.Part(text=user_message)]\n",
    "                )\n",
    "            ]\n",
    "        \n",
    "        \n",
    "        processed_input = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\", \n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=instruction,\n",
    "            ),\n",
    "            contents = contents\n",
    "        )\n",
    "        \n",
    "        match = re.search(r'\\{.*\\}', processed_input.text, re.DOTALL)\n",
    "        if match:\n",
    "            clean_json_str = match.group(0)\n",
    "            processed_input = json.loads(clean_json_str)\n",
    "     \n",
    "        return processed_input, pdf_text\n",
    "\n",
    "    \n",
    "    def _extract_text_from_pdf(self, path):\n",
    "        text = \"\"\n",
    "        with pymupdf.open(path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        return text\n",
    "        \n",
    "        \n",
    "    def _create_embedding(self, text)-> list:\n",
    "        print(f' - CALL: create_embedding({text[:20]}...)')\n",
    "        vector_embedding = client.models.embed_content(\n",
    "            model='models/text-embedding-004',\n",
    "            contents=text,\n",
    "            config=types.EmbedContentConfig(task_type='SEMANTIC_SIMILARITY')\n",
    "        )\n",
    "        return vector_embedding.embeddings[0].values\n",
    "    \n",
    "    def _search_embedded_documents(self, query_embedding:list[float], n:int)->list[str]:\n",
    "        print(f' - CALL: search_embedded_documents(n = {n})')\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n\n",
    "        )\n",
    "        return results\n",
    "    \n",
    "    def _retrieve_documents(self, doc_ids:list[int])-> list[dict]:\n",
    "        print(f' - CALL: retrieve_documents(IDS = {doc_ids})')\n",
    "        papers_retrieved = []\n",
    "        for doc_id in doc_ids:\n",
    "            papers_retrieved.append(papers[doc_id])\n",
    "            \n",
    "        return papers_retrieved\n",
    "        \n",
    "    def _generate_final_answer(self,question: str, user_document: str, search_output: str, extended_info: str):\n",
    "        instruction = \"\"\"\n",
    "            You are an advanced scientific research assistant tasked with providing comprehensive answers based on retrieved academic papers.\n",
    "            \n",
    "            CONTEXT:\n",
    "            - The user has asked a QUESTION about a scientific topic\n",
    "            - They've provided their own INPUT_DOCUMENT (a scientific paper or research proposal)\n",
    "            - You've retrieved relevant papers from a scientific database (EMBED_DATA and EXTENDED_PAPER_INFO)\n",
    "            \n",
    "            YOUR TASK:\n",
    "            1. Analyze the retrieved papers and determine their relevance to the question\n",
    "            2. Provide an answer based on the retreived papers from EMBED_DATA and EXTENDED_PAPER_INFO. \n",
    "            3. You may use information from the INPUT_DOCUMENT if the papers from EMBED_DATA and EXTENDED_PAPER_INFO are not relevant. Always mention where the information is obtained from.\n",
    "            \n",
    "            IMPORTANT GUIDELINES:\n",
    "            - SKIP the user's own paper if it appears in the results\n",
    "            - Prioritize recent papers and high-impact findings\n",
    "            - Compare and contrast contradictory findings when present\n",
    "            - Always provide authors and publication dates\n",
    "            - Focus on scientific significance rather than general summaries\n",
    "            - For methodology questions, emphasize technical details and implementation\n",
    "            - Use objective, academically-appropriate language\n",
    "            - Provide the answer in a Markdown format\n",
    "            - You do not need to show all the papers from EMBED_DATA and EXTENDED_PAPER_INFO, only the most relevant and important\n",
    "            - For the answer, use only papers from the database. You may include some suggestions to other papers but don't make it too extensive.\n",
    "            - If the numer of papers is explicit in the QUESTION, show only the amount asked\n",
    "            - Do not mention the amount of papers retrieved from EMBED_DATA and EXTENDED_PAPER_INFO\n",
    "            \"\"\"\n",
    "\n",
    "        if user_document != None:\n",
    "            prompt =f\"\"\"\n",
    "                    QUESTION:{user_message}\n",
    "                    EMBED_DATA: {search_output}\n",
    "                    EXTENDED_PAPER_INFO: {extended_info}\n",
    "                    \"\"\"\n",
    "        else: \n",
    "            prompt =f\"\"\"\n",
    "                    QUESTION:{user_message}\n",
    "                    INPUT_DOCUMENT:{user_document}\n",
    "                    EMBED_DATA: {search_output}\n",
    "                    EXTENDED_PAPER_INFO: {extended_info}\n",
    "                    \"\"\"\n",
    "        \n",
    "        contents = []\n",
    "        contents.append(types.Content(role=\"user\", parts=[types.Part(text = prompt)]))\n",
    "        response_final = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\", \n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=instruction\n",
    "            ),\n",
    "            contents = contents\n",
    "        )\n",
    "        return response_final.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed96c64",
   "metadata": {
    "papermill": {
     "duration": 0.008326,
     "end_time": "2025-04-13T19:38:28.069792",
     "exception": false,
     "start_time": "2025-04-13T19:38:28.061466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example Usage 1\n",
    "Find scientific papers related to an input paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e433e3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:38:28.088620Z",
     "iopub.status.busy": "2025-04-13T19:38:28.088259Z",
     "iopub.status.idle": "2025-04-13T19:38:33.008007Z",
     "shell.execute_reply": "2025-04-13T19:38:33.007018Z"
    },
    "papermill": {
     "duration": 4.931097,
     "end_time": "2025-04-13T19:38:33.009796",
     "exception": false,
     "start_time": "2025-04-13T19:38:28.078699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - CALL: create_embedding(Deep Reinforcement L...)\n",
      " - CALL: search_embedded_documents(n = 8)\n",
      " - CALL: retrieve_documents(IDS = [5587, 2885, 1469, 1229, 5050, 2242, 705, 2971])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are some papers that focus on uncertainty estimation in the context of reinforcement learning and autonomous systems:\n",
       "\n",
       "1.  **Zero-shot Model-based Reinforcement Learning using Large Language Models**\n",
       "\n",
       "    *   **Authors:** Abdelhakim Benechehab, et al.\n",
       "    *   **Publication Date:** February 2025\n",
       "    *   This paper explores the use of Large Language Models (LLMs) to predict the dynamics of continuous Markov decision processes in reinforcement learning. A key contribution is the demonstration that their approach produces well-calibrated uncertainty estimates, addressing a critical aspect of model-based RL. The method, called Disentangled In-Context Learning (DICL), tackles challenges related to handling multivariate data and incorporating control signals, which often limit the deployment of LLMs in continuous state spaces (Benechehab et al., 2025).\n",
       "2.  **Similarity-Distance-Magnitude Universal Verification**\n",
       "\n",
       "    *   **Author:** Allen Schmaltz\n",
       "    *   **Publication Date:** March 2025\n",
       "    *   The study introduces a novel approach to neural network robustness by incorporating similarity and distance awareness into the softmax function, resulting in the sdm activation function. This function provides strong signals of epistemic predictive uncertainty, which is crucial for model checking and decision-making. The uncertainty estimates from sdm calibration are robust to distribution shifts and out-of-distribution inputs, making them suitable for various applications, including selective classification and conditional branching (Schmaltz, 2025).\n",
       "3.  **Data-Efficient Reinforcement Learning with Probabilistic Model Predictive Control**\n",
       "\n",
       "    *   **Authors:** Sanket Kamthe and Marc Peter Deisenroth\n",
       "    *   **Publication Date:** February 2018\n",
       "    *   The paper introduces a model-based RL framework using probabilistic Model Predictive Control (MPC). It learns a probabilistic transition model using Gaussian Processes (GPs) to incorporate model uncertainty into long-term predictions, reducing the impact of model errors. The approach provides theoretical guarantees for first-order optimality in GP-based transition models with deterministic approximate inference for long-term planning and demonstrates state-of-the-art data efficiency (Kamthe & Deisenroth, 2018).\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "\n",
    "document = \"/kaggle/input/unc-paper/2409.10655v2.pdf\"\n",
    "user_message = \"Find me related papers with emphasis in uncertainty estimation.\"\n",
    "\n",
    "chatbot = RAG_Scientific_chatbot()\n",
    "answer = chatbot.chat(user_message, document)\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c6ca0",
   "metadata": {
    "papermill": {
     "duration": 0.008504,
     "end_time": "2025-04-13T19:38:33.027476",
     "exception": false,
     "start_time": "2025-04-13T19:38:33.018972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Example Usage 2\n",
    "Search papers related to a topic and specify the amount of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e037bd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:38:33.046291Z",
     "iopub.status.busy": "2025-04-13T19:38:33.045894Z",
     "iopub.status.idle": "2025-04-13T19:38:37.371261Z",
     "shell.execute_reply": "2025-04-13T19:38:37.370049Z"
    },
    "papermill": {
     "duration": 4.336872,
     "end_time": "2025-04-13T19:38:37.373025",
     "exception": false,
     "start_time": "2025-04-13T19:38:33.036153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - CALL: create_embedding(Robotics, machine le...)\n",
      " - CALL: search_embedded_documents(n = 6)\n",
      " - CALL: retrieve_documents(IDS = [8034, 7157, 3099, 8095, 4625, 7025])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are 6 papers that investigate robotics and machine learning:\n",
       "\n",
       "1.  **RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation** by Wang (2025) introduces RoboFlamingo-Plus, an enhancement to the RoboFlamingo framework that incorporates depth data into Vision-Language Models (VLMs) to improve robotic manipulation performance. The research integrates a pre-trained Vision Transformer (ViT) with a resampling technique for RGB and depth information fusion, aligning this data with linguistic cues for multimodal understanding. Experimental results indicate that RoboFlamingo-Plus improves robotic manipulation by 10-20% compared to current methods.\n",
       "2.  **Reinforcement Learning for UAV control with Policy and Reward Shaping** by Millán-Arias et al. (2022) explores the use of reinforcement learning (RL) to control a drone, employing reward-shaping and policy-shaping techniques. The study investigates two simulated scenarios: one without obstacles and one with obstacles, and analyzes the influence of each technique. The results indicate that an agent trained simultaneously with both techniques achieves lower execution times and less dispersion during training, despite obtaining a lower reward compared to an agent trained using only a policy-based approach.\n",
       "3.  **Information And Control: Insights from within the brain** by Dresp-Langley (2022) discusses the implications of the brain's neural networks for biologically inspired control structures in robotics. It explains how multisensory representations for action are generated based on signal input from vision, sound, smell, touch, and proprioception. The paper also details how the somatosensory cortex integrates sensory information for multimodal and multifunctional control of complex behaviors.\n",
       "4.  **A Survey on Human-aware Robot Navigation** by Möller et al. (2021) provides a survey of existing solutions for the navigation aspect of a socially-compliant robot and gives an outlook on possible future directions.\n",
       "5.  **Lvio-Fusion: A Self-adaptive Multi-sensor Fusion SLAM Framework Using Actor-critic Method** by Jia et al. (2021) introduces Lvio-Fusion, a tightly coupled multi-sensor fusion framework that fuses stereo camera, Lidar, IMU, and GPS data based on graph optimization. The system uses an actor-critic method in reinforcement learning to adaptively adjust sensors' weight, achieving high estimation accuracy and robustness in various environments.\n",
       "6.  **Deep Reinforcement Learning: An Overview** by Li (2018) presents an overview of recent achievements in deep reinforcement learning (RL), discussing core elements such as Deep Q-Network (DQN), policy, reward, model, planning, and exploration. The paper also covers important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Additionally, it explores various applications of RL, including robotics, games (e.g., AlphaGo), natural language processing, computer vision, and more."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "user_message = \"I am looking for papers 6 that investigate robotics and machine learning.\"\n",
    "\n",
    "chatbot = RAG_Scientific_chatbot()\n",
    "answer = chatbot.chat(user_message)\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28902a",
   "metadata": {
    "papermill": {
     "duration": 0.008652,
     "end_time": "2025-04-13T19:38:37.390755",
     "exception": false,
     "start_time": "2025-04-13T19:38:37.382103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Limitations\n",
    "If we search for x, y and z papers from three topics, the RAG cannot process this information. This is where an agent would be of great use, because it can call multiple times different functions and gather all the data one by one to provide an answer.\n",
    "\n",
    "Because of this, an agent was partially developed but the automatic calling of functions did not work correctly. In the appendix, partial code for this agent can be found.\n",
    "\n",
    "Furthermore, it cannot open documents in other format than PDF. This is also a field for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05f3c3b",
   "metadata": {
    "papermill": {
     "duration": 0.008809,
     "end_time": "2025-04-13T19:38:37.410099",
     "exception": false,
     "start_time": "2025-04-13T19:38:37.401290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Appendix: Partial code for an AI Agent \n",
    "\n",
    "This code is provided for future improvement, given that the implementation did not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16ba3abf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:38:37.430423Z",
     "iopub.status.busy": "2025-04-13T19:38:37.430047Z",
     "iopub.status.idle": "2025-04-13T19:38:37.436236Z",
     "shell.execute_reply": "2025-04-13T19:38:37.435126Z"
    },
    "papermill": {
     "duration": 0.018092,
     "end_time": "2025-04-13T19:38:37.438137",
     "exception": false,
     "start_time": "2025-04-13T19:38:37.420045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Tool declarations ===\n",
    "create_embedding_tool = {\n",
    "    \"name\" : \"create_embedding\",\n",
    "    \"description\" : \"For a given text, generate the corresponding vector embedding.\",\n",
    "    \"parameters\" : {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"The input text to embed.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"text\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "search_embedded_documents_tool = {\n",
    "    \"name\" : \"search_embedded_documents\",\n",
    "    \"description\" : \"Search for similar documents using a query embedding.\",\n",
    "    \"parameters\" : {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"query_embedding\": {\n",
    "                \"type\": \"ARRAY\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"NUMBER\"  # O \"INTEGER\" si tus vectores son int (normalmente son floats)\n",
    "                },\n",
    "                \"description\": \"The vector embedding of the input query.\"\n",
    "            },\n",
    "            \"n\": {\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"description\": \"Number of top similar documents to retrieve.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query_embedding\", \"n\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "retrieve_documents_tool = {\n",
    "    \"name\" : \"retrieve_documents\",\n",
    "    \"description\" : \"Retrieve detailed information about a document using its ID.\",\n",
    "    \"parameters\" : {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"description\": \"The ID of the paper/document.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"doc_id\"]\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03e436c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:38:37.457934Z",
     "iopub.status.busy": "2025-04-13T19:38:37.457515Z",
     "iopub.status.idle": "2025-04-13T19:38:37.462445Z",
     "shell.execute_reply": "2025-04-13T19:38:37.461390Z"
    },
    "papermill": {
     "duration": 0.016724,
     "end_time": "2025-04-13T19:38:37.464324",
     "exception": false,
     "start_time": "2025-04-13T19:38:37.447600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#orchestration_tools  = types.Tool(function_declarations=[create_embedding_tool, search_embedded_documents_tool])\n",
    "\n",
    "orchestration_tools  = types.Tool(function_declarations=[create_embedding_tool, search_embedded_documents_tool])\n",
    "\n",
    "\n",
    "instruction = \"\"\"You are a helpful chatbot that can interact with a database of vector embeddings\n",
    "of scientific papers and a database with the papers extended information. You will take the users questions and documents andgenerate\n",
    "\n",
    "Use the following tools:\n",
    "    - create_embedding(text) to convert text into vector embeddings \n",
    "    - search_embedded_documents(query_embedding, n) to obtain n papers that are similar to the embedded query \n",
    "from the database of vector embeddings.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "564804ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T19:38:37.492537Z",
     "iopub.status.busy": "2025-04-13T19:38:37.492096Z",
     "iopub.status.idle": "2025-04-13T19:38:37.496727Z",
     "shell.execute_reply": "2025-04-13T19:38:37.495732Z"
    },
    "papermill": {
     "duration": 0.017124,
     "end_time": "2025-04-13T19:38:37.498381",
     "exception": false,
     "start_time": "2025-04-13T19:38:37.481257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tool_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "#if tool_call.name == \"create_embedding\":\n",
    "#    result = create_embedding(**tool_call.args)\n",
    "\n",
    "#function_response_part = types.Part.from_function_response(\n",
    "#    name=tool_call.name,\n",
    "#    response={\"result\": result},\n",
    "#)\n",
    "\n",
    "#contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=tool_call)])) # Append the model's function call message\n",
    "#contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n",
    "#response = client.models.generate_content(\n",
    "#    model=\"gemini-2.0-flash\", \n",
    "#    config=types.GenerateContentConfig(\n",
    "#        system_instruction=instruction,\n",
    "#        tools=[orchestration_tools]\n",
    "#    ),\n",
    "#    contents = contents\n",
    "#)\n",
    "#print(response)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11814972,
     "datasetId": 612177,
     "sourceId": 11382540,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11757919,
     "datasetId": 7088951,
     "sourceId": 11332325,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 299.517499,
   "end_time": "2025-04-13T19:38:40.427872",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-13T19:33:40.910373",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
