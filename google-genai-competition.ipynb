{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad12df2",
   "metadata": {
    "papermill": {
     "duration": 0.006504,
     "end_time": "2025-04-12T17:28:52.072591",
     "exception": false,
     "start_time": "2025-04-12T17:28:52.066087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "Import and install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e4ba18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:28:52.085253Z",
     "iopub.status.busy": "2025-04-12T17:28:52.084912Z",
     "iopub.status.idle": "2025-04-12T17:29:44.039667Z",
     "shell.execute_reply": "2025-04-12T17:29:44.038674Z"
    },
    "papermill": {
     "duration": 51.963519,
     "end_time": "2025-04-12T17:29:44.041731",
     "exception": false,
     "start_time": "2025-04-12T17:28:52.078212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.9/454.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\r\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mCollecting pymupdf\r\n",
      "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\r\n",
      "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pymupdf\r\n",
      "Successfully installed pymupdf-1.25.5\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n",
    "!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\"\n",
    "!pip install --upgrade pymupdf\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a141b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:29:44.059224Z",
     "iopub.status.busy": "2025-04-12T17:29:44.058634Z",
     "iopub.status.idle": "2025-04-12T17:29:44.758022Z",
     "shell.execute_reply": "2025-04-12T17:29:44.756747Z"
    },
    "papermill": {
     "duration": 0.710594,
     "end_time": "2025-04-12T17:29:44.760141",
     "exception": false,
     "start_time": "2025-04-12T17:29:44.049547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API keys\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d82e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:29:44.777852Z",
     "iopub.status.busy": "2025-04-12T17:29:44.777484Z",
     "iopub.status.idle": "2025-04-12T17:29:45.036769Z",
     "shell.execute_reply": "2025-04-12T17:29:45.035620Z"
    },
    "papermill": {
     "duration": 0.270583,
     "end_time": "2025-04-12T17:29:45.038725",
     "exception": false,
     "start_time": "2025-04-12T17:29:44.768142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a retry policy. The model might make multiple consecutive calls automatically\n",
    "# for a complex query, this ensures the client retries if it hits quota limits.\n",
    "from google.api_core import retry\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "if not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n",
    "  genai.models.Models.generate_content = retry.Retry(\n",
    "      predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517c7cc",
   "metadata": {
    "papermill": {
     "duration": 0.007761,
     "end_time": "2025-04-12T17:29:45.054269",
     "exception": false,
     "start_time": "2025-04-12T17:29:45.046508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Upload of Arxiv papers\n",
    "First import the arxiv dataset and then perform vector embedding of all the documents. After the vector embedding, it is saved in a chromadb vector database. The arxiv dataset import is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4786e275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:29:45.071297Z",
     "iopub.status.busy": "2025-04-12T17:29:45.070780Z",
     "iopub.status.idle": "2025-04-12T17:29:45.349141Z",
     "shell.execute_reply": "2025-04-12T17:29:45.347922Z"
    },
    "papermill": {
     "duration": 0.289175,
     "end_time": "2025-04-12T17:29:45.351052",
     "exception": false,
     "start_time": "2025-04-12T17:29:45.061877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "amount_papers = 10000\n",
    "papers = []\n",
    "with open('/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json', 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= amount_papers:\n",
    "            break\n",
    "        papers.append(json.loads(line))\n",
    "\n",
    "# Now data is a list of dictionaries\n",
    "print(\"Headers:\", list(papers[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc9a6a",
   "metadata": {
    "papermill": {
     "duration": 0.007885,
     "end_time": "2025-04-12T17:29:45.366840",
     "exception": false,
     "start_time": "2025-04-12T17:29:45.358955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Only the title and the abstract of each paper will be embedded. The code below implements this preprocessing of the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13ed288a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:29:45.384671Z",
     "iopub.status.busy": "2025-04-12T17:29:45.384205Z",
     "iopub.status.idle": "2025-04-12T17:29:45.420119Z",
     "shell.execute_reply": "2025-04-12T17:29:45.418572Z"
    },
    "papermill": {
     "duration": 0.046939,
     "end_time": "2025-04-12T17:29:45.421991",
     "exception": false,
     "start_time": "2025-04-12T17:29:45.375052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFULLY PREPROCESSED 10000 PAPERS\n",
      "--- EXAMPLE OF PREPROCESSED PAPER ---\n",
      "PAPER TITLE: Calculation of prompt diphoton production cross sections at Tevatron and   LHC energies\n",
      "PAPER CONTENT:   A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is demonstrated with data from the Fermilab Tevatron, and predictions are made for more detailed tests with CDF and DO data. Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC, showing that enhanced sensitivity to the signal can be obtained with judicious selection of events. \n"
     ]
    }
   ],
   "source": [
    "def remove_newlines(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return obj.replace('\\n', ' ')\n",
    "        \n",
    "preprocessed_papers = []\n",
    "for paper in papers:\n",
    "    preprocessed_papers.append(\"PAPER TITLE: \" + remove_newlines(paper[\"title\"]) + \"\\nPAPER CONTENT: \"+ remove_newlines(paper[\"abstract\"]))\n",
    "print(\"SUCCESSFULLY PREPROCESSED \"+ str(len(preprocessed_papers)) + \" PAPERS\")\n",
    "print(\"--- EXAMPLE OF PREPROCESSED PAPER ---\")\n",
    "print(preprocessed_papers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752b86f",
   "metadata": {
    "papermill": {
     "duration": 0.007648,
     "end_time": "2025-04-12T17:29:45.438017",
     "exception": false,
     "start_time": "2025-04-12T17:29:45.430369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now the preprocessed papers are transformed into vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ee9253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:29:45.455129Z",
     "iopub.status.busy": "2025-04-12T17:29:45.454724Z",
     "iopub.status.idle": "2025-04-12T17:32:07.955500Z",
     "shell.execute_reply": "2025-04-12T17:32:07.954118Z"
    },
    "papermill": {
     "duration": 142.52001,
     "end_time": "2025-04-12T17:32:07.965988",
     "exception": false,
     "start_time": "2025-04-12T17:29:45.445978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFULLY EMBEDDED 10000 PAPERS\n"
     ]
    }
   ],
   "source": [
    "def batch(iterable, n=100):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i + n]\n",
    "\n",
    "papers_embedded = []  \n",
    "papers_batches = list(batch(preprocessed_papers, 100)) #limit of 100 embeddings per call\n",
    "\n",
    "for batch in papers_batches:\n",
    "    batch_embedded = client.models.embed_content(\n",
    "        model='models/text-embedding-004',\n",
    "        contents=batch,\n",
    "        config=types.EmbedContentConfig(task_type='SEMANTIC_SIMILARITY'))\n",
    "    list_batch_embedded = [e.values for e in batch_embedded.embeddings]\n",
    "    papers_embedded+=list_batch_embedded\n",
    "\n",
    "print(\"SUCCESSFULLY EMBEDDED \"+ str(len(papers_embedded)) + \" PAPERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5d846",
   "metadata": {
    "papermill": {
     "duration": 0.009994,
     "end_time": "2025-04-12T17:32:07.984055",
     "exception": false,
     "start_time": "2025-04-12T17:32:07.974061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Once the vector embeddings of the papers are computed, these are stored into the chromadb database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed69dd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:32:08.002025Z",
     "iopub.status.busy": "2025-04-12T17:32:08.001668Z",
     "iopub.status.idle": "2025-04-12T17:32:26.974239Z",
     "shell.execute_reply": "2025-04-12T17:32:26.973046Z"
    },
    "papermill": {
     "duration": 18.984058,
     "end_time": "2025-04-12T17:32:26.976440",
     "exception": false,
     "start_time": "2025-04-12T17:32:07.992382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFULLY UPLOADED 10000 PAPERS\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "def batch(iterable, batch_size):\n",
    "    for i in range(0, len(iterable), batch_size):\n",
    "        yield iterable[i:i + batch_size]\n",
    "\n",
    "\n",
    "# Start ChromaDB client\n",
    "chromadb_client = chromadb.Client()\n",
    "\n",
    "# Create or get a collection\n",
    "collection = chromadb_client.get_or_create_collection(name=\"papers\")\n",
    "\n",
    "# Add the documents + embeddings to Chroma\n",
    "emb_batches = list(batch(papers_embedded, 41000))\n",
    "papers_batches = list(batch(preprocessed_papers, 41000))\n",
    "for i in range(len(emb_batches)):\n",
    "    ids_batch = [f\"doc_{j + i * 41000}\" for j in range(len(emb_batches[i]))]\n",
    "    collection.add(\n",
    "        documents=papers_batches[i],\n",
    "        embeddings=emb_batches[i],\n",
    "        ids=ids_batch,\n",
    "    )\n",
    "print(\"SUCCESSFULLY UPLOADED \"+ str(len(papers_embedded)) + \" PAPERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2384c",
   "metadata": {
    "papermill": {
     "duration": 0.007487,
     "end_time": "2025-04-12T17:32:26.992021",
     "exception": false,
     "start_time": "2025-04-12T17:32:26.984534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vector database search example\n",
    "\n",
    "Now an example paper is used to search for similar papers in the database. If the same paper is obtained, the queried paper was in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb5329b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:32:27.010057Z",
     "iopub.status.busy": "2025-04-12T17:32:27.009552Z",
     "iopub.status.idle": "2025-04-12T17:32:27.433274Z",
     "shell.execute_reply": "2025-04-12T17:32:27.432197Z"
    },
    "papermill": {
     "duration": 0.435386,
     "end_time": "2025-04-12T17:32:27.435437",
     "exception": false,
     "start_time": "2025-04-12T17:32:27.000051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_input = \"Statistical modeling of experimental physical laws is based on the probability density function of measured variables. It is expressed by experimental data via a kernel estimator. The kernel is determined objectively by the scattering of data during calibration of experimental setup. A physical law, which relates measured variables, is optimally extracted from experimental data by the conditional average estimator. It is derived directly from the kernel estimator and corresponds to a general nonparametric regression. T\"\n",
    "#query_input = pdf_text\n",
    "\n",
    "query_embedding = client.models.embed_content(\n",
    "        model='models/text-embedding-004',\n",
    "        contents=query_input,\n",
    "        config=types.EmbedContentConfig(task_type='SEMANTIC_SIMILARITY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7539915c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:32:27.452591Z",
     "iopub.status.busy": "2025-04-12T17:32:27.452210Z",
     "iopub.status.idle": "2025-04-12T17:32:27.465261Z",
     "shell.execute_reply": "2025-04-12T17:32:27.464108Z"
    },
    "papermill": {
     "duration": 0.023602,
     "end_time": "2025-04-12T17:32:27.466973",
     "exception": false,
     "start_time": "2025-04-12T17:32:27.443371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: doc_88\n",
      "PAPER TITLE: A general approach to statistical modeling of physical laws:   nonparametric regression\n",
      "PAPER CONTENT:   Statistical modeling of experimental physical laws is based on the probability density function of measured variables. It is expressed by experimental data via a kernel estimator. The kernel is determined objectively by the scattering of data during calibration of experimental setup. A physical law, which relates measured variables, is optimally extracted from experimental data by the conditional average estimator. It is derived directly from the kernel estimator and corresponds to a general nonparametric regression. The proposed method is demonstrated by the modeling of a return map of noisy chaotic data. In this example, the nonparametric regression is used to predict a future value of chaotic time series from the present one. The mean predictor error is used in the definition of predictor quality, while the redundancy is expressed by the mean square distance between data points. Both statistics are used in a new definition of predictor cost function. From the minimum of the predictor cost function, a proper number of data in the model is estimated. \n",
      "\n",
      "ID: doc_106\n",
      "PAPER TITLE: Experimental modeling of physical laws\n",
      "PAPER CONTENT:   A physical law is represented by the probability distribution of a measured variable. The probability density is described by measured data using an estimator whose kernel is the instrument scattering function. The experimental information and data redundancy are defined in terms of information entropy. The model cost function, comprised of data redundancy and estimation error, is minimized by the creation-annihilation process. \n",
      "\n",
      "ID: doc_150\n",
      "PAPER TITLE: Extraction of physical laws from joint experimental data\n",
      "PAPER CONTENT:   The extraction of a physical law y=yo(x) from joint experimental data about x and y is treated. The joint, the marginal and the conditional probability density functions (PDF) are expressed by given data over an estimator whose kernel is the instrument scattering function. As an optimal estimator of yo(x) the conditional average is proposed. The analysis of its properties is based upon a new definition of prediction quality. The joint experimental information and the redundancy of joint measurements are expressed by the relative entropy. With the number of experiments the redundancy on average increases, while the experimental information converges to a certain limit value. The difference between this limit value and the experimental information at a finite number of data represents the discrepancy between the experimentally determined and the true properties of the phenomenon. The sum of the discrepancy measure and the redundancy is utilized as a cost function. By its minimum a reasonable number of data for the extraction of the law yo(x) is specified. The mutual information is defined by the marginal and the conditional PDFs of the variables. The ratio between mutual information and marginal information is used to indicate which variable is the independent one. The properties of the introduced statistics are demonstrated on deterministically and randomly related variables. \n",
      "\n",
      "ID: doc_6776\n",
      "PAPER TITLE: Some Aspects of Measurement Error in Linear Regression of Astronomical   Data\n",
      "PAPER CONTENT:   I describe a Bayesian method to account for measurement errors in linear regression of astronomical data. The method allows for heteroscedastic and possibly correlated measurement errors, and intrinsic scatter in the regression relationship. The method is based on deriving a likelihood function for the measured data, and I focus on the case when the intrinsic distribution of the independent variables can be approximated using a mixture of Gaussians. I generalize the method to incorporate multiple independent variables, non-detections, and selection effects (e.g., Malmquist bias). A Gibbs sampler is described for simulating random draws from the probability distribution of the parameters, given the observed data. I use simulation to compare the method with other common estimators. The simulations illustrate that the Gaussian mixture model outperforms other common estimators and can effectively give constraints on the regression parameters, even when the measurement errors dominate the observed scatter, source detection fraction is low, or the intrinsic distribution of the independent variables is not a mixture of Gaussians. I conclude by using this method to fit the X-ray spectral slope as a function of Eddington ratio using a sample of 39 z < 0.8 radio-quiet quasars. I confirm the correlation seen by other authors between the radio-quiet quasar X-ray spectral slope and the Eddington ratio, where the X-ray spectral slope softens as the Eddington ratio increases. \n",
      "\n",
      "ID: doc_670\n",
      "PAPER TITLE: Learning from compressed observations\n",
      "PAPER CONTENT:   The problem of statistical learning is to construct a predictor of a random variable $Y$ as a function of a related random variable $X$ on the basis of an i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable predictors are drawn from some specified class, and the goal is to approach asymptotically the performance (expected loss) of the best predictor in the class. We consider the setting in which one has perfect observation of the $X$-part of the sample, while the $Y$-part has to be communicated at some finite bit rate. The encoding of the $Y$-values is allowed to depend on the $X$-values. Under suitable regularity conditions on the admissible predictors, the underlying family of probability distributions and the loss function, we give an information-theoretic characterization of achievable predictor performance in terms of conditional distortion-rate functions. The ideas are illustrated on the example of nonparametric regression in Gaussian noise. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding.embeddings[0].values],\n",
    "    n_results=5  # Number of similar docs to return\n",
    ")\n",
    "\n",
    "for doc, doc_id in zip(results[\"documents\"][0], results[\"ids\"][0]):\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8420d01",
   "metadata": {
    "papermill": {
     "duration": 0.007745,
     "end_time": "2025-04-12T17:32:27.482961",
     "exception": false,
     "start_time": "2025-04-12T17:32:27.475216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RETRIEVAL AUGMENTED GENERATION (RAG)\n",
    "For retrieval augmented generation, the question of the user together with the document are used to search for useful papers. With the useful papers and the user input, an answer is generated. The steps are as follow:\n",
    "1) Use a LLM to embed the user input question and input document for vector search.\n",
    "2) Obtain the original documents from the vector search in the database.\n",
    "3) Use the input question and input document and the original documents from the database to generate a response with a LLM.\n",
    "4) Show the answer to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a6649",
   "metadata": {
    "papermill": {
     "duration": 0.00791,
     "end_time": "2025-04-12T17:32:27.498896",
     "exception": false,
     "start_time": "2025-04-12T17:32:27.490986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Orchestration functions\n",
    "The following functions orchestrate the RAG:\n",
    "\n",
    "- create_embedding(text): For a given text generates the corresponding vector embedding.\n",
    "- search_embedded_documents(query_embedding, n): For a given vector, searches nearby vectors in the vector embeddings database.\n",
    "- retrieve_documents(doc_id): For a given list of document ids, this function returns an extended information of each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f926da5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:32:27.516877Z",
     "iopub.status.busy": "2025-04-12T17:32:27.516396Z",
     "iopub.status.idle": "2025-04-12T17:32:27.523495Z",
     "shell.execute_reply": "2025-04-12T17:32:27.522067Z"
    },
    "papermill": {
     "duration": 0.017929,
     "end_time": "2025-04-12T17:32:27.525341",
     "exception": false,
     "start_time": "2025-04-12T17:32:27.507412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# === Tools ===\n",
    "def create_embedding(text)-> list:\n",
    "    print(f' - CALL: create_embedding({text[:20]})')\n",
    "    vector_embedding = client.models.embed_content(\n",
    "        model='models/text-embedding-004',\n",
    "        contents=text,\n",
    "        config=types.EmbedContentConfig(task_type='SEMANTIC_SIMILARITY')\n",
    "    )\n",
    "    return vector_embedding.embeddings[0].values\n",
    "\n",
    "def search_embedded_documents(query_embedding:list[float], n:int)->list[str]:\n",
    "    print(f' - CALL: search_embedded_documents(n = {n})')\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def retrieve_documents(doc_ids:list[int])-> list[dict]:\n",
    "    print(f' - CALL: retrieve_documents(IDS = {doc_ids})')\n",
    "    papers_retrieved = []\n",
    "    for doc_id in doc_ids:\n",
    "        papers_retrieved.append(papers[doc_id])\n",
    "        \n",
    "    return papers_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd801a52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:32:27.543741Z",
     "iopub.status.busy": "2025-04-12T17:32:27.543336Z",
     "iopub.status.idle": "2025-04-12T17:32:27.996115Z",
     "shell.execute_reply": "2025-04-12T17:32:27.994953Z"
    },
    "papermill": {
     "duration": 0.463714,
     "end_time": "2025-04-12T17:32:27.998072",
     "exception": false,
     "start_time": "2025-04-12T17:32:27.534358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "def extract_text_from_pdf(path):\n",
    "    text = \"\"\n",
    "    with pymupdf.open(path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"/kaggle/input/unc-paper/2409.10655v2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8341efeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:32:28.016319Z",
     "iopub.status.busy": "2025-04-12T17:32:28.015979Z",
     "iopub.status.idle": "2025-04-12T17:32:29.394272Z",
     "shell.execute_reply": "2025-04-12T17:32:29.393054Z"
    },
    "papermill": {
     "duration": 1.389908,
     "end_time": "2025-04-12T17:32:29.396284",
     "exception": false,
     "start_time": "2025-04-12T17:32:28.006376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "user_document = pdf_text[:1000]\n",
    "user_message = \"Find me related papers.\"\n",
    "\n",
    "instruction = \"\"\"\n",
    "You are a helpful chatbot that processes inputs from users and generates an output JSON for vector search. \n",
    "\n",
    "Given a user message and a document, return:\n",
    "{\n",
    "  \"embedding_query\": \"<summarized embedding query based on user message and document>\",\n",
    "  \"num_documents\": <integer number of documents to retrieve>\n",
    "}\n",
    "\n",
    "Ensure the output is a valid JSON object. 'embedding_query' should be a concise string that captures the main topic or keywords for semantic search. 'num_documents' should be inferred from the user message, defaulting to 5 if unspecified.\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    types.Content(\n",
    "        role=\"user\", parts=[types.Part(text=user_message),types.Part(text=pdf_text)]\n",
    "    )\n",
    "]\n",
    "\n",
    "response_init = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", \n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=instruction,\n",
    "        #tools=[orchestration_tools]\n",
    "    ),\n",
    "    contents = contents\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2edde9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:32:29.415028Z",
     "iopub.status.busy": "2025-04-12T17:32:29.414527Z",
     "iopub.status.idle": "2025-04-12T17:32:29.429316Z",
     "shell.execute_reply": "2025-04-12T17:32:29.427724Z"
    },
    "papermill": {
     "duration": 0.026207,
     "end_time": "2025-04-12T17:32:29.431293",
     "exception": false,
     "start_time": "2025-04-12T17:32:29.405086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import re\n",
    "\n",
    "class RAG_Scientific_chatbot:\n",
    "    def chat(self, question:str, document_path:str):\n",
    "               \n",
    "        processed_input, user_document = self._process_input(question, document_path)\n",
    "    \n",
    "        embedding = self._create_embedding(processed_input[\"embedding_query\"])\n",
    "        \n",
    "        search_output = self._search_embedded_documents(embedding, int(processed_input[\"num_documents\"]))\n",
    "        \n",
    "        doc_ids = search_output['ids'][0]  \n",
    "        numeric_ids = [int(doc.split('_')[1]) for doc in doc_ids]\n",
    "        extended_info = self._retrieve_documents(numeric_ids)\n",
    "        answer = self._generate_final_answer(question, user_document, search_output, extended_info)\n",
    "\n",
    "        return answer\n",
    "\n",
    "\n",
    "    # === Tools ===\n",
    "    def _process_input(self, user_message: str, document_path:str):\n",
    "        instruction = \"\"\"\n",
    "        You are a helpful chatbot that processes inputs from users and generates an output JSON for vector search. \n",
    "        \n",
    "        Given a user message and a document, return this string output:\n",
    "        \n",
    "        {\"embedding_query\": \"<summarized embedding query based on user message and document>\",\n",
    "         \"num_documents\": \"<integer number of documents to retrieve>\"}\n",
    "        \n",
    "        'embedding_query' should be a concise string that captures the main topic or keywords for semantic search. 'num_documents' should be inferred from the user message, defaulting to 5 if unspecified.\n",
    "        \"\"\"\n",
    "\n",
    "        pdf_text = self._extract_text_from_pdf(document_path)\n",
    "        \n",
    "        contents = [\n",
    "            types.Content(\n",
    "                role=\"user\", parts=[types.Part(text=user_message),types.Part(text=pdf_text)]\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        processed_input = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\", \n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=instruction,\n",
    "            ),\n",
    "            contents = contents\n",
    "        )\n",
    "        \n",
    "        match = re.search(r'\\{.*\\}', processed_input.text, re.DOTALL)\n",
    "        if match:\n",
    "            clean_json_str = match.group(0)\n",
    "            processed_input = json.loads(clean_json_str)\n",
    "     \n",
    "        return processed_input, pdf_text\n",
    "\n",
    "    \n",
    "    def _extract_text_from_pdf(self, path):\n",
    "        text = \"\"\n",
    "        with pymupdf.open(path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        return text\n",
    "        \n",
    "        \n",
    "    def _create_embedding(self, text)-> list:\n",
    "        print(f' - CALL: create_embedding({text[:20]}...)')\n",
    "        vector_embedding = client.models.embed_content(\n",
    "            model='models/text-embedding-004',\n",
    "            contents=text,\n",
    "            config=types.EmbedContentConfig(task_type='SEMANTIC_SIMILARITY')\n",
    "        )\n",
    "        return vector_embedding.embeddings[0].values\n",
    "    \n",
    "    def _search_embedded_documents(self, query_embedding:list[float], n:int)->list[str]:\n",
    "        print(f' - CALL: search_embedded_documents(n = {n})')\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n\n",
    "        )\n",
    "        return results\n",
    "    \n",
    "    def _retrieve_documents(self, doc_ids:list[int])-> list[dict]:\n",
    "        print(f' - CALL: retrieve_documents(IDS = {doc_ids})')\n",
    "        papers_retrieved = []\n",
    "        for doc_id in doc_ids:\n",
    "            papers_retrieved.append(papers[doc_id])\n",
    "            \n",
    "        return papers_retrieved\n",
    "        \n",
    "    def _generate_final_answer(self,question: str, user_document: str, search_output: str, extended_info: str):\n",
    "        instruction = \"\"\"You are a helpful chatbot that uses retrieval augmented generation to answer questions regarding scientific papers. The user provided a QUESTION and an INPUT_DOCUMENT. \n",
    "                The information retreived from the database is composed of: \n",
    "                - EMBED_DATA: Shows the documents obtained after a vector search from the embedding of QUESTION and INPUT_DOCUMENT and the database.\n",
    "                - EXTENDED_PAPER_INFO: Shows more information of the papers from EMBED_DATA.\n",
    "\n",
    "                Answer the user QUESTION using the EMBED_DATA and EXTENDED_PAPER_INFO. IF the EMBED_DATA has the paper from INPUT_DOCUMENT, skip the paper in the answer.\n",
    "                Show always the author names and the publishing date of the papers.\n",
    "                \"\"\"\n",
    "\n",
    "        prompt =f\"\"\"\n",
    "                QUESTION:{user_message}\n",
    "                INPUT_DOCUMENT:{user_document}\n",
    "                EMBED_DATA: {search_output}\n",
    "                EXTENDED_PAPER_INFO: {extended_info}\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        contents = []\n",
    "        contents.append(types.Content(role=\"user\", parts=[types.Part(text = prompt)]))\n",
    "        response_final = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\", \n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=instruction\n",
    "            ),\n",
    "            contents = contents\n",
    "        )\n",
    "        return response_final.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d7e107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:32:29.450338Z",
     "iopub.status.busy": "2025-04-12T17:32:29.449940Z",
     "iopub.status.idle": "2025-04-12T17:32:34.090228Z",
     "shell.execute_reply": "2025-04-12T17:32:34.089003Z"
    },
    "papermill": {
     "duration": 4.651633,
     "end_time": "2025-04-12T17:32:34.092110",
     "exception": false,
     "start_time": "2025-04-12T17:32:29.440477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - CALL: create_embedding(Safe social navigati...)\n",
      " - CALL: search_embedded_documents(n = 5)\n",
      " - CALL: retrieve_documents(IDS = [4604, 3375, 4438, 1554, 1273])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are some related papers:\n",
       "\n",
       "*   **Risk Assessment Algorithms Based On Recursive Neural Networks** by Alejandro Chinea Manrique De Lara and Michel Parent (2007). This paper introduces a novel approach to compute risk functions by using a combination of a highly non-linear processing model in conjunction with a powerful information encoding procedure.\n",
       "*   **Flow of autonomous traffic on a single multi-lane street** by Federico Polito and Fergal Dalton (2007). This paper investigates the behavior of an original traffic model that considers a single multi-lane street populated by autonomous vehicles.\n",
       "*   **Mixing navigation on networks** by Tao Zhou (2007). This paper proposes a mixing navigation mechanism that interpolates between random-walk and shortest-path protocol to enhance navigation efficiency.\n",
       "*   **An information-based traffic control in a public conveyance system: reduced clustering and enhanced efficiency** by A. Tomoeda, K. Nishinari, D. Chowdhury and A. Schadschneider (2007). This paper proposes a new public conveyance model using stochastic cellular automaton to find the optimal density of vehicles and reduce clustering.\n",
       "*   **Parametric Learning and Monte Carlo Optimization** by David H. Wolpert and Dev G. Rajnarayan (2007). This paper uncovers the close relationship between Monte Carlo Optimization, Parametric machine-Learning, and blackbox optimization.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "\n",
    "document = \"/kaggle/input/unc-paper/2409.10655v2.pdf\"\n",
    "user_message = \"Find me related papers.\"\n",
    "\n",
    "chatbot = RAG_Scientific_chatbot()\n",
    "answer = chatbot.chat(user_message, document)\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb87fd",
   "metadata": {
    "papermill": {
     "duration": 0.008071,
     "end_time": "2025-04-12T17:32:34.109224",
     "exception": false,
     "start_time": "2025-04-12T17:32:34.101153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Appendix: Partial code for an AI Agent \n",
    "\n",
    "This code is provided for future improvement, given that the code did not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfc7e983",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:32:34.127975Z",
     "iopub.status.busy": "2025-04-12T17:32:34.127508Z",
     "iopub.status.idle": "2025-04-12T17:32:34.136062Z",
     "shell.execute_reply": "2025-04-12T17:32:34.134914Z"
    },
    "papermill": {
     "duration": 0.020102,
     "end_time": "2025-04-12T17:32:34.137927",
     "exception": false,
     "start_time": "2025-04-12T17:32:34.117825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Tool declarations ===\n",
    "create_embedding_tool = {\n",
    "    \"name\" : \"create_embedding\",\n",
    "    \"description\" : \"For a given text, generate the corresponding vector embedding.\",\n",
    "    \"parameters\" : {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"The input text to embed.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"text\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "search_embedded_documents_tool = {\n",
    "    \"name\" : \"search_embedded_documents\",\n",
    "    \"description\" : \"Search for similar documents using a query embedding.\",\n",
    "    \"parameters\" : {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"query_embedding\": {\n",
    "                \"type\": \"ARRAY\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"NUMBER\"  # O \"INTEGER\" si tus vectores son int (normalmente son floats)\n",
    "                },\n",
    "                \"description\": \"The vector embedding of the input query.\"\n",
    "            },\n",
    "            \"n\": {\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"description\": \"Number of top similar documents to retrieve.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query_embedding\", \"n\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "retrieve_documents_tool = {\n",
    "    \"name\" : \"retrieve_documents\",\n",
    "    \"description\" : \"Retrieve detailed information about a document using its ID.\",\n",
    "    \"parameters\" : {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\n",
    "                \"type\": \"INTEGER\",\n",
    "                \"description\": \"The ID of the paper/document.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"doc_id\"]\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ebdb190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:32:34.156930Z",
     "iopub.status.busy": "2025-04-12T17:32:34.156393Z",
     "iopub.status.idle": "2025-04-12T17:32:34.162095Z",
     "shell.execute_reply": "2025-04-12T17:32:34.160994Z"
    },
    "papermill": {
     "duration": 0.017111,
     "end_time": "2025-04-12T17:32:34.164032",
     "exception": false,
     "start_time": "2025-04-12T17:32:34.146921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#orchestration_tools  = types.Tool(function_declarations=[create_embedding_tool, search_embedded_documents_tool])\n",
    "\n",
    "orchestration_tools  = types.Tool(function_declarations=[create_embedding_tool, search_embedded_documents_tool])\n",
    "\n",
    "\n",
    "instruction = \"\"\"You are a helpful chatbot that can interact with a database of vector embeddings\n",
    "of scientific papers and a database with the papers extended information. You will take the users questions and documents andgenerate\n",
    "\n",
    "Use the following tools:\n",
    "    - create_embedding(text) to convert text into vector embeddings \n",
    "    - search_embedded_documents(query_embedding, n) to obtain n papers that are similar to the embedded query \n",
    "from the database of vector embeddings.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be998399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T17:32:34.182544Z",
     "iopub.status.busy": "2025-04-12T17:32:34.181925Z",
     "iopub.status.idle": "2025-04-12T17:32:34.186920Z",
     "shell.execute_reply": "2025-04-12T17:32:34.185798Z"
    },
    "papermill": {
     "duration": 0.016574,
     "end_time": "2025-04-12T17:32:34.189049",
     "exception": false,
     "start_time": "2025-04-12T17:32:34.172475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tool_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "#if tool_call.name == \"create_embedding\":\n",
    "#    result = create_embedding(**tool_call.args)\n",
    "\n",
    "#function_response_part = types.Part.from_function_response(\n",
    "#    name=tool_call.name,\n",
    "#    response={\"result\": result},\n",
    "#)\n",
    "\n",
    "#contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=tool_call)])) # Append the model's function call message\n",
    "#contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n",
    "#response = client.models.generate_content(\n",
    "#    model=\"gemini-2.0-flash\", \n",
    "#    config=types.GenerateContentConfig(\n",
    "#        system_instruction=instruction,\n",
    "#        tools=[orchestration_tools]\n",
    "#    ),\n",
    "#    contents = contents\n",
    "#)\n",
    "#print(response)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 612177,
     "sourceId": 11291024,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7088951,
     "sourceId": 11332325,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 227.37166,
   "end_time": "2025-04-12T17:32:35.821856",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-12T17:28:48.450196",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
