{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11291024,"sourceType":"datasetVersion","datasetId":612177},{"sourceId":11332325,"sourceType":"datasetVersion","datasetId":7088951}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Scientific Paper Recommendation System with RAG\n## Project Description\nThis project implements a Retrieval Augmented Generation (RAG) system for scientific paper recommendations. The system allows users to input a document or query and receive recommendations for relevant scientific papers from the ArXiv database.\n\n## Key Components\nVector Database: Uses a ChromaDB vector database containing embeddings of 10,000 ArXiv research papers\nDocument Processing: Extracts and processes PDF content using PyMuPDF\nSemantic Search: Performs similarity searches based on document content\n\n## GenAI Functionalities\n- Embeddings: Generates semantic embeddings using Google's text-embedding-004 model\n- Prompt Engineering: Utilizes carefully crafted prompts to guide the AI's behavior\n- RAG Implementation: Combines vector search results with generative AI responses\n- Document Understanding: Processes and interprets PDF research papers\n- Vector Embedding and Vector Search: Performs semantic similarity searches in high-dimensional vector space\n\nThe system orchestrates these components through a chatbot interface that processes user queries, searches for relevant papers, and generates comprehensive responses that include paper details like authors and publication dates.\n\n## Database\nArXiv serves as an excellent data source for our recommendation system for several key reasons:\n\n- **Rich Scientific Content**: Contains over 2 million scholarly articles across multiple disciplines\n- **Well-Structured Metadata**: Includes titles, abstracts, authors, and categories in a consistent format\n- **Embedding-Friendly**: Abstracts provide concise, information-dense text that produces meaningful vector embeddings\n- **Research Relevance**: Widely used by the scientific community, ensuring practical utility\n- **Semantic Search Compatibility**: Content structure works effectively with our embedding model (text-embedding-004)\n\nOur implementation uses 10,000 ArXiv papers converted to vector embeddings and stored in ChromaDB, enabling semantic similarity searches to retrieve relevant scientific literature for user queries. The Arxiv dataset is available in Kagggle with the following link: [Arxiv_dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv). \n## Use Case\nResearchers can upload a scientific paper and ask questions to find related work in the ArXiv database, facilitating literature reviews and discovery of relevant research.\n\n````\nchatbot = RAG_Scientific_chatbot()\nanswer = chatbot.chat(\"Find me related papers\", \"/path/to/document.pdf\")\ndisplay(Markdown(answer))\n```` \n","metadata":{}},{"cell_type":"markdown","source":"## Setup\nImport and install the necessary libraries.","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\"\n!pip install --upgrade pymupdf\n\nfrom google import genai\nfrom google.genai import types\n\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:47:03.459284Z","iopub.execute_input":"2025-04-12T20:47:03.459528Z","iopub.status.idle":"2025-04-12T20:47:52.106094Z","shell.execute_reply.started":"2025-04-12T20:47:03.459505Z","shell.execute_reply":"2025-04-12T20:47:52.105008Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.9/454.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting pymupdf\n  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\nDownloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pymupdf\nSuccessfully installed pymupdf-1.25.5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# API keys\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:48:22.033765Z","iopub.execute_input":"2025-04-12T20:48:22.034197Z","iopub.status.idle":"2025-04-12T20:48:22.721947Z","shell.execute_reply.started":"2025-04-12T20:48:22.034168Z","shell.execute_reply":"2025-04-12T20:48:22.720863Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Define a retry policy. The model might make multiple consecutive calls automatically\n# for a complex query, this ensures the client retries if it hits quota limits.\nfrom google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n  genai.models.Models.generate_content = retry.Retry(\n      predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:48:24.160384Z","iopub.execute_input":"2025-04-12T20:48:24.160770Z","iopub.status.idle":"2025-04-12T20:48:24.417043Z","shell.execute_reply.started":"2025-04-12T20:48:24.160742Z","shell.execute_reply":"2025-04-12T20:48:24.415959Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Upload of Arxiv papers\nFirst import the arxiv dataset and then perform vector embedding of all the documents. After the vector embedding, it is saved in a chromadb vector database. The arxiv dataset import is shown below.\n\nRANDOM SAMPLING","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\namount_papers = 10000\npapers = []\n\nwith open('/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json', 'r') as file:\n    for i, line in enumerate(file):\n        papers.append(json.loads(line))\n\nrandom_indices = set(random.sample(range(len(papers)), amount_papers))\nrandom_indices = list(random_indices)\npapers_random = []\nfor i in range(len(random_indices)):\n    index = random_indices[i]\n    papers_random.append(papers[index])\npapers = papers_random\n# Now data is a list of dictionaries\nprint(\"Headers:\", list(papers[0].keys()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T21:31:07.130498Z","iopub.execute_input":"2025-04-12T21:31:07.130865Z","iopub.status.idle":"2025-04-12T21:32:44.398096Z","shell.execute_reply.started":"2025-04-12T21:31:07.130839Z","shell.execute_reply":"2025-04-12T21:32:44.396681Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-05e802f50827>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mrandom_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount_papers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpapers\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpapers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpapers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# Now data is a list of dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Headers:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"],"ename":"IndexError","evalue":"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices","output_type":"error"}],"execution_count":49},{"cell_type":"markdown","source":"Only the title and the abstract of each paper will be embedded. The code below implements this preprocessing of the papers.","metadata":{}},{"cell_type":"code","source":"def remove_newlines(obj):\n    if isinstance(obj, str):\n        return obj.replace('\\n', ' ')\n        \npreprocessed_papers = []\nfor paper in papers:\n    preprocessed_papers.append(\"PAPER TITLE: \" + remove_newlines(paper[\"title\"]) + \"\\nPAPER CONTENT: \"+ remove_newlines(paper[\"abstract\"]))\nprint(\"SUCCESSFULLY PREPROCESSED \"+ str(len(preprocessed_papers)) + \" PAPERS\")\nprint(\"--- EXAMPLE OF PREPROCESSED PAPER ---\")\nprint(preprocessed_papers[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T21:40:38.571368Z","iopub.execute_input":"2025-04-12T21:40:38.571730Z","iopub.status.idle":"2025-04-12T21:40:40.227058Z","shell.execute_reply.started":"2025-04-12T21:40:38.571707Z","shell.execute_reply":"2025-04-12T21:40:40.226046Z"}},"outputs":[{"name":"stdout","text":"SUCCESSFULLY PREPROCESSED 100000 PAPERS\n--- EXAMPLE OF PREPROCESSED PAPER ---\nPAPER TITLE: Reputation for Playing Mixed Actions: A Characterization Theorem\nPAPER CONTENT:   A patient player privately observes a persistent state that directly affects his myopic opponents' payoffs, and can be one of the several commitment types that plays the same mixed action in every period. I characterize the set of environments under which the patient player obtains at least his commitment payoff in all equilibria regardless of his stage-game payoff function. Due to interdependent values, the patient player cannot guarantee his mixed commitment payoff by imitating the mixed-strategy commitment type, and small perturbations to a pure commitment action can significantly reduce the patient player's guaranteed equilibrium payoff. \n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"Now the preprocessed papers are transformed into vector embeddings.","metadata":{}},{"cell_type":"code","source":"def batch(iterable, n=100):\n    for i in range(0, len(iterable), n):\n        yield iterable[i:i + n]\n\npapers_embedded = []  \npapers_batches = list(batch(preprocessed_papers, 100)) #limit of 100 embeddings per call\n\nfor batch in papers_batches:\n    batch_embedded = client.models.embed_content(\n        model='models/text-embedding-004',\n        contents=batch,\n        config=types.EmbedContentConfig(task_type='SEMANTIC_SIMILARITY'))\n    list_batch_embedded = [e.values for e in batch_embedded.embeddings]\n    papers_embedded+=list_batch_embedded\n\nprint(\"SUCCESSFULLY EMBEDDED \"+ str(len(papers_embedded)) + \" PAPERS\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T21:40:49.003295Z","iopub.execute_input":"2025-04-12T21:40:49.003640Z","iopub.status.idle":"2025-04-12T22:00:03.633375Z","shell.execute_reply.started":"2025-04-12T21:40:49.003613Z","shell.execute_reply":"2025-04-12T22:00:03.631533Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-495f30a3f8a7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpapers_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     batch_embedded = client.models.embed_content(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models/text-embedding-004'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36membed_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4497\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4499\u001b[0;31m     response_dict = self._api_client.request(\n\u001b[0m\u001b[1;32m   4500\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4501\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     )\n\u001b[0;32m--> 640\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    560\u001b[0m       )\n\u001b[1;32m    561\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m       response = self._httpx_client.request(\n\u001b[0m\u001b[1;32m    563\u001b[0m           \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m           \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         )\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m     def _send_handling_auth(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \"\"\"\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_content\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36miter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mByteChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mraw_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m                     \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36miter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mraw_stream_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_bytes_downloaded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_stream_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_stream_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_httpcore_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mShieldCancellation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"receive_response_body\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":61},{"cell_type":"markdown","source":"Once the vector embeddings of the papers are computed, these are stored into the chromadb database.","metadata":{}},{"cell_type":"code","source":"import chromadb\nfrom chromadb import Documents, EmbeddingFunction, Embeddings\ndef batch(iterable, batch_size):\n    for i in range(0, len(iterable), batch_size):\n        yield iterable[i:i + batch_size]\n\n\n# Start ChromaDB client\nchromadb_client = chromadb.Client()\n\n# Create or get a collection\ncollection = chromadb_client.get_or_create_collection(name=\"papers\")\n\n# Add the documents + embeddings to Chroma\nemb_batches = list(batch(papers_embedded, 41000))\npapers_batches = list(batch(preprocessed_papers, 41000))\nfor i in range(len(emb_batches)):\n    ids_batch = [f\"doc_{j + i * 41000}\" for j in range(len(emb_batches[i]))]\n    collection.add(\n        documents=papers_batches[i],\n        embeddings=emb_batches[i],\n        ids=ids_batch,\n    )\nprint(\"SUCCESSFULLY UPLOADED \"+ str(len(papers_embedded)) + \" PAPERS\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:52:46.055494Z","iopub.execute_input":"2025-04-12T20:52:46.055841Z","iopub.status.idle":"2025-04-12T20:52:53.013312Z","shell.execute_reply.started":"2025-04-12T20:52:46.055813Z","shell.execute_reply":"2025-04-12T20:52:53.012268Z"}},"outputs":[{"name":"stdout","text":"SUCCESSFULLY UPLOADED 10000 PAPERS\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Vector database search example\n\nNow an example paper is used to search for similar papers in the database. If the same paper is obtained, the queried paper was in the database.","metadata":{}},{"cell_type":"code","source":"query_input = \"Statistical modeling of experimental physical laws is based on the probability density function of measured variables. It is expressed by experimental data via a kernel estimator. The kernel is determined objectively by the scattering of data during calibration of experimental setup. A physical law, which relates measured variables, is optimally extracted from experimental data by the conditional average estimator. It is derived directly from the kernel estimator and corresponds to a general nonparametric regression. T\"\n#query_input = pdf_text\n\nquery_embedding = client.models.embed_content(\n        model='models/text-embedding-004',\n        contents=query_input,\n        config=types.EmbedContentConfig(task_type='SEMANTIC_SIMILARITY'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T20:52:57.359560Z","iopub.execute_input":"2025-04-12T20:52:57.359939Z","iopub.status.idle":"2025-04-12T20:52:58.621134Z","shell.execute_reply.started":"2025-04-12T20:52:57.359909Z","shell.execute_reply":"2025-04-12T20:52:58.619802Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"results = collection.query(\n    query_embeddings=[query_embedding.embeddings[0].values],\n    n_results=5  # Number of similar docs to return\n)\n\nfor doc, doc_id in zip(results[\"documents\"][0], results[\"ids\"][0]):\n    print(f\"ID: {doc_id}\")\n    print(f\"{doc}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:18:05.111468Z","iopub.execute_input":"2025-04-12T17:18:05.111831Z","iopub.status.idle":"2025-04-12T17:18:05.127357Z","shell.execute_reply.started":"2025-04-12T17:18:05.111796Z","shell.execute_reply":"2025-04-12T17:18:05.126281Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RETRIEVAL AUGMENTED GENERATION (RAG)\nFor retrieval augmented generation, the question of the user together with the document are used to search for useful papers. With the useful papers and the user input, an answer is generated. The steps are as follow:\n1) Use a LLM to embed the user input question and input document for vector search.\n2) Obtain the original documents from the vector search in the database.\n3) Use the input question and input document and the original documents from the database to generate a response with a LLM.\n4) Show the answer to the user.","metadata":{}},{"cell_type":"markdown","source":"### Orchestration functions\nThe following functions orchestrate the RAG:\n\n- create_embedding(text): For a given text generates the corresponding vector embedding.\n- search_embedded_documents(query_embedding, n): For a given vector, searches nearby vectors in the vector embeddings database.\n- retrieve_documents(doc_id): For a given list of document ids, this function returns an extended information of each paper.","metadata":{}},{"cell_type":"code","source":"from google.genai import types\n\n# === Tools ===\ndef create_embedding(text)-> list:\n    print(f' - CALL: create_embedding({text[:20]})')\n    vector_embedding = client.models.embed_content(\n        model='models/text-embedding-004',\n        contents=text,\n        config=types.EmbedContentConfig(task_type='SEMANTIC_SIMILARITY')\n    )\n    return vector_embedding.embeddings[0].values\n\ndef search_embedded_documents(query_embedding:list[float], n:int)->list[str]:\n    print(f' - CALL: search_embedded_documents(n = {n})')\n    results = collection.query(\n        query_embeddings=[query_embedding],\n        n_results=n\n    )\n    return results\n\ndef retrieve_documents(doc_ids:list[int])-> list[dict]:\n    print(f' - CALL: retrieve_documents(IDS = {doc_ids})')\n    papers_retrieved = []\n    for doc_id in doc_ids:\n        papers_retrieved.append(papers[doc_id])\n        \n    return papers_retrieved","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:18:05.128584Z","iopub.execute_input":"2025-04-12T17:18:05.128958Z","iopub.status.idle":"2025-04-12T17:18:05.137798Z","shell.execute_reply.started":"2025-04-12T17:18:05.128925Z","shell.execute_reply":"2025-04-12T17:18:05.136521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pymupdf\n\ndef extract_text_from_pdf(path):\n    text = \"\"\n    with pymupdf.open(path) as doc:\n        for page in doc:\n            text += page.get_text()\n    return text\n\n\npdf_text = extract_text_from_pdf(\"/kaggle/input/unc-paper/2409.10655v2.pdf\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:18:05.139066Z","iopub.execute_input":"2025-04-12T17:18:05.139467Z","iopub.status.idle":"2025-04-12T17:18:05.468750Z","shell.execute_reply.started":"2025-04-12T17:18:05.139437Z","shell.execute_reply":"2025-04-12T17:18:05.467403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nuser_document = pdf_text[:1000]\nuser_message = \"Find me related papers.\"\n\ninstruction = \"\"\"\nYou are a helpful chatbot that processes inputs from users and generates an output JSON for vector search. \n\nGiven a user message and a document, return:\n{\n  \"embedding_query\": \"<summarized embedding query based on user message and document>\",\n  \"num_documents\": <integer number of documents to retrieve>\n}\n\nEnsure the output is a valid JSON object. 'embedding_query' should be a concise string that captures the main topic or keywords for semantic search. 'num_documents' should be inferred from the user message, defaulting to 5 if unspecified.\n\"\"\"\n\ncontents = [\n    types.Content(\n        role=\"user\", parts=[types.Part(text=user_message),types.Part(text=pdf_text)]\n    )\n]\n\nresponse_init = client.models.generate_content(\n    model=\"gemini-2.0-flash\", \n    config=types.GenerateContentConfig(\n        system_instruction=instruction,\n        #tools=[orchestration_tools]\n    ),\n    contents = contents\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:24:20.327338Z","iopub.execute_input":"2025-04-12T17:24:20.327725Z","iopub.status.idle":"2025-04-12T17:24:22.324506Z","shell.execute_reply.started":"2025-04-12T17:24:20.327693Z","shell.execute_reply":"2025-04-12T17:24:22.323143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pymupdf\nimport re\n\nclass RAG_Scientific_chatbot:\n        \n    def chat(self, question:str, document_path:str):\n               \n        processed_input, user_document = self._process_input(question, document_path)\n    \n        embedding = self._create_embedding(processed_input[\"embedding_query\"])\n        \n        search_output = self._search_embedded_documents(embedding, int(processed_input[\"num_documents\"]))\n        \n        doc_ids = search_output['ids'][0]  \n        numeric_ids = [int(doc.split('_')[1]) for doc in doc_ids]\n        extended_info = self._retrieve_documents(numeric_ids)\n        answer = self._generate_final_answer(question, user_document, search_output, extended_info)\n\n        return answer\n\n\n    # === Tools ===\n    def _process_input(self, user_message: str, document_path:str):\n        instruction = \"\"\"\n            You are a scientific research assistant specializing in analyzing academic papers and research questions.\n            \n            Your task is to analyze the user's question and their uploaded document to create:\n            1. An optimal embedding query for retrieving the most relevant scientific papers\n            2. A recommendation for how many papers to retrieve\n            \n            INSTRUCTIONS:\n            - Identify key scientific concepts, methodologies, domain-specific terminology, and research areas\n            - Extract specific technical terms that would appear in related papers\n            - Consider both the user's explicit question and the implicit research goals from their document\n            - Focus on scientific significance rather than general terms\n            - For empirical research questions, include methodology terms and measurement concepts\n            - For theoretical questions, include relevant frameworks and paradigms\n            \n            For number of documents:\n            - Suggest 3-5 papers for focused questions with specific methodology/technology\n            - Suggest 5-8 papers for broader research areas requiring multiple perspectives\n            - Suggest 8-12 papers for literature reviews or comparative analyses\n            \n            Return this string output:\n            \"{\"embedding_query\": \"<your optimized embedding query>\",\n             \"num_documents\": \"<number of papers to retrieve>\"}\"\n            \n            EXAMPLES:\n            Poor embedding query: \"machine learning effects\"\n            Good embedding query: \"transformer neural networks performance metrics BERT GPT comparative analysis NLP benchmarks\"\n            \"\"\"\n\n        pdf_text = self._extract_text_from_pdf(document_path)\n        \n        contents = [\n            types.Content(\n                role=\"user\", parts=[types.Part(text=user_message),types.Part(text=pdf_text)]\n            )\n        ]\n        \n        processed_input = client.models.generate_content(\n            model=\"gemini-2.0-flash\", \n            config=types.GenerateContentConfig(\n                system_instruction=instruction,\n            ),\n            contents = contents\n        )\n        \n        match = re.search(r'\\{.*\\}', processed_input.text, re.DOTALL)\n        if match:\n            clean_json_str = match.group(0)\n            processed_input = json.loads(clean_json_str)\n     \n        return processed_input, pdf_text\n\n    \n    def _extract_text_from_pdf(self, path):\n        text = \"\"\n        with pymupdf.open(path) as doc:\n            for page in doc:\n                text += page.get_text()\n        return text\n        \n        \n    def _create_embedding(self, text)-> list:\n        print(f' - CALL: create_embedding({text[:20]}...)')\n        vector_embedding = client.models.embed_content(\n            model='models/text-embedding-004',\n            contents=text,\n            config=types.EmbedContentConfig(task_type='SEMANTIC_SIMILARITY')\n        )\n        return vector_embedding.embeddings[0].values\n    \n    def _search_embedded_documents(self, query_embedding:list[float], n:int)->list[str]:\n        print(f' - CALL: search_embedded_documents(n = {n})')\n        results = collection.query(\n            query_embeddings=[query_embedding],\n            n_results=n\n        )\n        return results\n    \n    def _retrieve_documents(self, doc_ids:list[int])-> list[dict]:\n        print(f' - CALL: retrieve_documents(IDS = {doc_ids})')\n        papers_retrieved = []\n        for doc_id in doc_ids:\n            papers_retrieved.append(papers[doc_id])\n            \n        return papers_retrieved\n        \n    def _generate_final_answer(self,question: str, user_document: str, search_output: str, extended_info: str):\n        instruction = \"\"\"\n            You are an advanced scientific research assistant tasked with providing comprehensive answers based on retrieved academic papers.\n            \n            CONTEXT:\n            - The user has asked a QUESTION about a scientific topic\n            - They've provided their own INPUT_DOCUMENT (a scientific paper or research proposal)\n            - You've retrieved relevant papers from a scientific database (EMBED_DATA and EXTENDED_PAPER_INFO)\n            \n            YOUR TASK:\n            1. Analyze the retrieved papers and determine their relevance to the question\n            2. Provide an answer based on the retreived papers from EMBED_DATA and EXTENDED_PAPER_INFO. \n            3. You may use information from the INPUT_DOCUMENT if the papers from EMBED_DATA and EXTENDED_PAPER_INFO are not relevant. Always mention where the information is obtained from.\n            \n            IMPORTANT GUIDELINES:\n            - SKIP the user's own paper if it appears in the results\n            - Prioritize recent papers and high-impact findings\n            - Compare and contrast contradictory findings when present\n            - Always provide authors and publication dates\n            - Focus on scientific significance rather than general summaries\n            - For methodology questions, emphasize technical details and implementation\n            - Use objective, academically-appropriate language\n            - Provide the answer in a Markdown format\n            - You do not need to show all the papers from EMBED_DATA and EXTENDED_PAPER_INFO, only the most relevant and important\n            - For the answer, use only papers from the database. You may include some suggestions to other papers but don't make it too extensive.\n            \"\"\"\n\n\n        prompt =f\"\"\"\n                QUESTION:{user_message}\n                INPUT_DOCUMENT:{user_document}\n                EMBED_DATA: {search_output}\n                EXTENDED_PAPER_INFO: {extended_info}\n                \"\"\"\n        \n        contents = []\n        contents.append(types.Content(role=\"user\", parts=[types.Part(text = prompt)]))\n        response_final = client.models.generate_content(\n            model=\"gemini-2.0-flash\", \n            config=types.GenerateContentConfig(\n                system_instruction=instruction\n            ),\n            contents = contents\n        )\n        return response_final.text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T21:22:46.135356Z","iopub.execute_input":"2025-04-12T21:22:46.135748Z","iopub.status.idle":"2025-04-12T21:22:46.149861Z","shell.execute_reply.started":"2025-04-12T21:22:46.135722Z","shell.execute_reply":"2025-04-12T21:22:46.148674Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from IPython.display import display, Markdown, Latex\n\n\ndocument = \"/kaggle/input/unc-paper/2409.10655v2.pdf\"\nuser_message = \"Find me related papers with emphasis in uncertainty estimation.\"\n\nchatbot = RAG_Scientific_chatbot()\nanswer = chatbot.chat(user_message, document)\ndisplay(Markdown(answer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T21:22:49.098253Z","iopub.execute_input":"2025-04-12T21:22:49.098609Z","iopub.status.idle":"2025-04-12T21:22:54.961159Z","shell.execute_reply.started":"2025-04-12T21:22:49.098581Z","shell.execute_reply":"2025-04-12T21:22:54.959771Z"}},"outputs":[{"name":"stdout","text":" - CALL: create_embedding(deep reinforcement l...)\n - CALL: search_embedded_documents(n = 7)\n - CALL: retrieve_documents(IDS = [4604, 4763, 393, 5682, 89, 3432, 7484])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Based on the provided papers, here's an overview of research related to uncertainty estimation:\n\n*   **Risk Assessment Algorithms Based On Recursive Neural Networks** by Chinea Manrique De Lara and Parent (2007) introduces a novel approach to compute risk functions using a combination of a highly non-linear processing model in conjunction with a powerful information encoding procedure. The elements of information either static or dynamic that appear in a road intersection scene are encoded by using directed positional acyclic labeled graphs. The risk assessment problem is then reformulated in terms of an inductive learning task carried out by a recursive neural network.\n\n*   **Bayesian Approach to Neuro-Rough Models** by Marwala and Crossingham (2007) proposes a neuro-rough model based on multi-layered perceptron and rough set. The neuro-rough model is then tested on modelling the risk of HIV from demographic data. The model is formulated using Bayesian framework and trained using Monte Carlo method and Metropolis criterion.\n\n*   **Option Pricing Using Bayesian Neural Networks** by Pires and Marwala (2007) explores option pricing using Bayesian Neural Networks. They use two techniques for Bayesian neural networks: Automatic Relevance Determination (for Gaussian Approximation) and a Hybrid Monte Carlo method, both used with Multi-Layer Perceptrons.\n\n*   **Bayesian approach to rough set** by Marwala and Crossingham (2007) proposes an approach to training rough set models using Bayesian framework trained using Markov Chain Monte Carlo (MCMC) method. The prior probabilities are constructed from the prior knowledge that good rough set models have fewer rules.\n\nThese papers use Bayesian and neural network approaches for risk assessment and uncertainty handling in different contexts."},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"if answer and isinstance(answer, str):\n    display(Markdown(answer))\nelse:\n    print(\"Received non-string response:\", type(answer))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T21:02:47.883034Z","iopub.execute_input":"2025-04-12T21:02:47.883441Z","iopub.status.idle":"2025-04-12T21:02:47.890455Z","shell.execute_reply.started":"2025-04-12T21:02:47.883413Z","shell.execute_reply":"2025-04-12T21:02:47.889425Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```markdown\n## Related Papers for Safe Social Navigation using Deep Reinforcement Learning\n\nHere's a summary of papers related to your work on disentangling uncertainty for safe social navigation using deep reinforcement learning (DRL).\n\n**Direct Answer:**\n\nThe retrieved papers cover a range of topics related to risk assessment, navigation, and learning in complex environments. Several papers address uncertainty and risk in different contexts, while others focus on navigation strategies and learning algorithms.\n\n**Key Insights:**\n\n*   **Risk Assessment using Neural Networks:** One paper explores using recursive neural networks for risk assessment at road intersections, highlighting the importance of learning the structure of risk (Manrique De Lara & Parent, 2007).\n*   **Risk-Sensitive Control:** Another paper deals with Markov control processes and establishes optimality criteria for risk-sensitive average cost (Jaśkiewicz, 2007).\n*   **Learning to Bluff:** A paper investigates how intelligent agents can learn to bluff in games by predicting opponents' reactions and optimizing returns through statistical methods (Hurwitz & Marwala, 2007). This relates to social interaction and decision-making under uncertainty.\n*   **Mixing Navigation Strategies:** One paper proposes a mixing navigation mechanism that combines random walk and shortest-path protocols to enhance navigation efficiency in networks (Zhou, 2008).\n*   **Neuro-Rough Models:** A paper introduces a neuro-rough model combining multi-layered perceptrons and rough set theory, applied to modeling the risk of HIV infection (Marwala & Crossingham, 2007).\n\n**Methodological Approaches:**\n\n*   **Recursive Neural Networks:** Used for learning risk structures in dynamic environments (Manrique De Lara & Parent, 2007).\n*   **Markov Control Processes:** Applied for risk-sensitive control with average cost criteria (Jaśkiewicz, 2007).\n*   **Adaptive Simulated Annealing:** Used in project scheduling to optimize parameters and sample probability distributions (Ingber, 2007).\n*   **TD-Lambda Learning:** Employed for training agents to learn bluffing strategies in games (Hurwitz & Marwala, 2007).\n*   **Bayesian Framework:** Used in neuro-rough models for risk assessment, trained with Monte Carlo methods (Marwala & Crossingham, 2007).\n\n**Paper Summaries:**\n\n1.  **Title:** Risk Assessment Algorithms Based On Recursive Neural Networks\n    *   **Authors:** Alejandro Chinea Manrique De Lara and Michel Parent (2007)\n    *   **Key Contributions:** Introduces a novel approach to compute risk functions using recursive neural networks and directed positional acyclic labeled graphs to encode information in road intersection scenes. The model learns the structure of risk.\n    *   **Methodological Strengths:** Utilizes a combination of a non-linear processing model with a powerful information encoding procedure.\n2.  **Title:** Average optimality for risk-sensitive control with general state space\n    *   **Author:** Anna Jaśkiewicz (2007)\n    *   **Key Contributions:** Establishes the optimality inequality and an optimal stationary strategy for risk-sensitive control in discrete-time Markov control processes with a general state space.\n    *   **Methodological Strengths:** Uses the vanishing discount factor approach for long-run risk-sensitive average cost criterion.\n3.  **Title:** Real Options for Project Schedules (ROPS)\n    *   **Author:** Lester Ingber (2007)\n    *   **Key Contributions:** Presents a project scheduling method (ROPS) with three recursive sampling/optimization shells using Adaptive Simulated Annealing (ASA) to optimize parameters of strategic plans.\n    *   **Methodological Strengths:** Employs a multi-shell approach to sample probability distributions of durations and costs of tasks.\n4.  **Title:** Learning to Bluff\n    *   **Authors:** Evan Hurwitz and Tshilidzi Marwala (2007)\n    *   **Key Contributions:** Demonstrates how intelligent agents can learn to bluff by predicting opponents' reactions and using statistical optimization.\n    *   **Methodological Strengths:** Uses a TD-Lambda learning algorithm to continuously adapt neural network agent intelligence.\n5.  **Title:** Maximizing the Growth Rate under Risk Constraints\n    *   **Authors:** Traian A. Pirvu and Gordan Zitkovic (2007)\n    *   **Key Contributions:** Investigates growth-rate maximization under risk constraints in financial markets, showing that the optimal policy can be obtained by scaling down the unconstrained optimal portfolio.\n6.  **Title:** Mixing navigation on networks\n    *   **Author:** Tao Zhou (2008)\n    *   **Key Contributions:** Proposes a mixing navigation mechanism that interpolates between random-walk and shortest-path protocols to enhance navigation efficiency in networks.\n    *   **Methodological Strengths:** Uses targeted and clustering strategies to improve efficiency and reduce communication costs.\n7.  **Title:** Bayesian Approach to Neuro-Rough Models\n    *   **Authors:** Tshilidzi Marwala and Bodie Crossingham (2007)\n    *   **Key Contributions:** Proposes a neuro-rough model based on multi-layered perceptron and rough set theory, formulated using a Bayesian framework and trained with Monte Carlo methods.\n    *   **Methodological Strengths:** Combines the accuracy of Bayesian MLP models and the transparency of Bayesian rough set models.\n\n**Research Gaps:**\n\n*   While some papers address risk assessment and navigation, they do not explicitly focus on *disentangling* different types of uncertainty (aleatoric, epistemic) in the context of DRL for social navigation.\n*   The application of recursive neural networks for risk assessment in road intersections (Manrique De Lara & Parent, 2007) could be explored further in the context of robot navigation in pedestrian-rich environments.\n*   The \"Learning to Bluff\" paper (Hurwitz & Marwala, 2007) suggests interesting approaches to modeling social interaction and decision-making under uncertainty, which could be relevant to socially-aware robot navigation.\n```"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"## Appendix: Partial code for an AI Agent \n\nThis code is provided for future improvement, given that the code did not work","metadata":{}},{"cell_type":"code","source":"# === Tool declarations ===\ncreate_embedding_tool = {\n    \"name\" : \"create_embedding\",\n    \"description\" : \"For a given text, generate the corresponding vector embedding.\",\n    \"parameters\" : {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n            \"text\": {\n                \"type\": \"STRING\",\n                \"description\": \"The input text to embed.\"\n            }\n        },\n        \"required\": [\"text\"]\n    }\n}\n\nsearch_embedded_documents_tool = {\n    \"name\" : \"search_embedded_documents\",\n    \"description\" : \"Search for similar documents using a query embedding.\",\n    \"parameters\" : {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n            \"query_embedding\": {\n                \"type\": \"ARRAY\",\n                \"items\": {\n                    \"type\": \"NUMBER\"  # O \"INTEGER\" si tus vectores son int (normalmente son floats)\n                },\n                \"description\": \"The vector embedding of the input query.\"\n            },\n            \"n\": {\n                \"type\": \"INTEGER\",\n                \"description\": \"Number of top similar documents to retrieve.\"\n            }\n        },\n        \"required\": [\"query_embedding\", \"n\"]\n    }\n}\n\nretrieve_documents_tool = {\n    \"name\" : \"retrieve_documents\",\n    \"description\" : \"Retrieve detailed information about a document using its ID.\",\n    \"parameters\" : {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n            \"doc_id\": {\n                \"type\": \"INTEGER\",\n                \"description\": \"The ID of the paper/document.\"\n            }\n        },\n        \"required\": [\"doc_id\"]\n    }\n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:18:05.494699Z","iopub.status.idle":"2025-04-12T17:18:05.495081Z","shell.execute_reply":"2025-04-12T17:18:05.494913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#orchestration_tools  = types.Tool(function_declarations=[create_embedding_tool, search_embedded_documents_tool])\n\norchestration_tools  = types.Tool(function_declarations=[create_embedding_tool, search_embedded_documents_tool])\n\n\ninstruction = \"\"\"You are a helpful chatbot that can interact with a database of vector embeddings\nof scientific papers and a database with the papers extended information. You will take the users questions and documents andgenerate\n\nUse the following tools:\n    - create_embedding(text) to convert text into vector embeddings \n    - search_embedded_documents(query_embedding, n) to obtain n papers that are similar to the embedded query \nfrom the database of vector embeddings.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:18:05.495831Z","iopub.status.idle":"2025-04-12T17:18:05.496137Z","shell.execute_reply":"2025-04-12T17:18:05.496010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#tool_call = response.candidates[0].content.parts[0].function_call\n\n#if tool_call.name == \"create_embedding\":\n#    result = create_embedding(**tool_call.args)\n\n#function_response_part = types.Part.from_function_response(\n#    name=tool_call.name,\n#    response={\"result\": result},\n#)\n\n#contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=tool_call)])) # Append the model's function call message\n#contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n#response = client.models.generate_content(\n#    model=\"gemini-2.0-flash\", \n#    config=types.GenerateContentConfig(\n#        system_instruction=instruction,\n#        tools=[orchestration_tools]\n#    ),\n#    contents = contents\n#)\n#print(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T17:18:05.496926Z","iopub.status.idle":"2025-04-12T17:18:05.497239Z","shell.execute_reply":"2025-04-12T17:18:05.497116Z"}},"outputs":[],"execution_count":null}]}